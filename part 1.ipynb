{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Supervised Learining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"http://www0.cs.ucl.ac.uk/staff/M.Herbster/SL/misc/zipcombo.dat\"\n",
    "training_filename = 'dtrain123.dat'\n",
    "test_filename = 'dtest123.dat'\n",
    "filename = 'zipcombo.dat'\n",
    "\n",
    "# If we have already downloaded the file then read from it\n",
    "if os.path.isfile(filename): \n",
    "    data = np.loadtxt(filename)\n",
    "else:\n",
    "    # Otherwise, just download it and then load it in memory\n",
    "    urllib.request.urlretrieve(link, filename)\n",
    "    data = np.loadtxt(filename)\n",
    "    \n",
    "y = data[:,0]\n",
    "x = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Perceptron with Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Polynomial_Kernel(x1,x2,d):\n",
    "    \"\"\"\n",
    "    Constructions the polynomial kernel matrix of order d\n",
    "    :param x1: the first set of observations\n",
    "    :param x2: the second set of observations\n",
    "    :param d: the order of polynomial\n",
    "    :return: the polynomial kernel\n",
    "    \"\"\"\n",
    "    K = (x1 @ x2.T)**d\n",
    "    return K\n",
    "\n",
    "def transform_y(y):\n",
    "    \"\"\"\n",
    "    Helper function used in perceptron weight updating. This transforms y from a vector containing digits\n",
    "    into a matrix where each row i represents an observation and each column j denotes with {-1,1} whether the\n",
    "    observation i label was of column j.\n",
    "    :param y: the label vector y\n",
    "    :return: a transformed y matrix\n",
    "    \"\"\"\n",
    "    classes_num = 10\n",
    "    m = len(y)\n",
    "    y_matrix = np.ones((m,classes_num))*(-1)\n",
    "    for i in range(m):\n",
    "        y_matrix[i,int(y[i])] = 1\n",
    "    return y_matrix\n",
    "\n",
    "def pairwise_distance_single(X): # distances of X training data, single X matrix\n",
    "    \"\"\"\n",
    "    Computes the pairwise distance for a single X matrix\n",
    "    :param X: the matrix for which we need to compute the pairwise distances\n",
    "    :return: the pairwise distances matrix\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    G = np.matmul(X,X.T)\n",
    "    DG = np.diag(G).reshape(G.shape[0],1)\n",
    "    distances_sq = np.matmul(DG,np.ones((G.shape[0],1)).T)+ np.matmul(np.ones((G.shape[1],1)),DG.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def pairwise_distance_double(X1,X2): # distances of X training data, double matrices, X1 and X2\n",
    "    \"\"\"\n",
    "    Computes the pairwise distance for two matrices X1 and X2\n",
    "    :param X1: the first matrix\n",
    "    :param X2: the second matrix\n",
    "    :return: the pairwise distances matrix\n",
    "    \"\"\"\n",
    "    X1_pow = (X1**2).sum(axis=1).reshape(X1.shape[0],1) #sum the rows, size m1 array\n",
    "    X2_pow = (X2**2).sum(axis=1).reshape(X2.shape[0],1) #sum the rows, size m2 array\n",
    "    G = np.matmul(X1,X2.T)\n",
    "    m1,m2 =G.shape[0],G.shape[1] \n",
    "    distances_sq = np.matmul(X1_pow,np.ones((m2,1)).T)+ np.matmul(np.ones((m1,1)),X2_pow.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def Gaussian_Kernel(distances_sq,c=1):\n",
    "    \"\"\"\n",
    "    Given a matrix which contains the pairwise distances, it calculates the gaussian kernel\n",
    "    :param distances_sq: the pairwise distances matrix\n",
    "    :param c: the constant scale of the exponential factor\n",
    "    :return: the kernel matrix K\n",
    "    \"\"\"\n",
    "    K = np.exp(-c*distances_sq)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kernel_single(x, d, kernel_choice):\n",
    "    \"\"\"\n",
    "    Calculates the kernel function of matrix x.\n",
    "    :param x: the observations matrix\n",
    "    :param d: is either the order of the polynomial kernel or the constant in the gaussian kernel\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the kernel matrix\n",
    "    \"\"\"\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x,x,d)\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_single(x)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "def calculate_kernel_double(x1, x2, d, kernel_choice):\n",
    "    \"\"\"\n",
    "    Calculates the kernel function of matrix x1 and x2.\n",
    "    :param x1: the first matrix x1\n",
    "    :param x2: the first matrix x2\n",
    "    :param d: is either the order of the polynomial kernel or the constant in the gaussian kernel\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the kernel matrix\n",
    "    \"\"\"\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x1,x2,d)\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_double(x1, x2)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "def perceptron_epoch(x, y, y_arr, alpha, K_train):\n",
    "    \"\"\"\n",
    "    Given a set of weights alpha, this function goes through the whole set of examples in x, one by one\n",
    "    and iteratively update the weights if a mistake has happened.\n",
    "    :param x: the observations array\n",
    "    :param y: the labels vector\n",
    "    :param y_arr: the transformed labels matrix\n",
    "    :param alpha: the set of weights\n",
    "    :param K_train: the kernel matrix as calculated over x\n",
    "    :return: the updated weights alpha and the number of errors made while going through the points\n",
    "    \"\"\"\n",
    "    # Number of examples\n",
    "    m = x.shape[0] \n",
    "    errors = np.zeros(m)\n",
    "    num_errors = 0\n",
    "\n",
    "    for t in range(m):\n",
    "        # Find our training set\n",
    "        x_t = x[t,:] #n size (1,n)\n",
    "        y_t = y[t]\n",
    "        y_arr_t = y_arr[t,:] # size (1,10) \n",
    "\n",
    "        # pred_t computes \\sum^{t-1}_{i=0} {(alpha_i K(x_t, x_i))}, \n",
    "        # which is regarded as the confidence in each class\n",
    "        pred_t = (alpha[:,:].T @K_train[t,:]).T\n",
    "        y_hat_t = np.where(pred_t == max(pred_t), 1, 0) # map the confidence to arrays of 1 and 0 for class\n",
    "\n",
    "        if pred_t.argmax() != y_t:\n",
    "            # Update the alpha, and weights, for all the classes that not the true class\n",
    "            num_errors +=1\n",
    "\n",
    "            # Note that alpha_t is np.zeros(10,1), \n",
    "            # and alpha_t is updated according to the real class, and the misclassified class.\n",
    "            alpha_t = alpha[t,:] + np.where(y_arr_t> 0,1,0) + np.where(y_hat_t>0,-1,0) #(1,10)\n",
    "\n",
    "            # Store alpha_t into the matrix for future reference\n",
    "            alpha[t,:] = alpha_t\n",
    "\n",
    "            # Sandwich K(x_t, x_i) for i in [1,t-1] in a zeros array of size(m). \n",
    "            # Reason being weight for one class is of size(m), \n",
    "            # but we only 'have enough data' to update the first t-1 terms.  \n",
    "            K_update = np.zeros((1,m))\n",
    "            K_update[:,:t] = K_train[t,:t]\n",
    "\n",
    "        errors[t] = num_errors\n",
    "    return alpha, errors\n",
    "\n",
    "# One vs. rest: train k classifiers to identify k classes\n",
    "def perceptron_train(x, y, d=2, kernel_choice='Polynomial', convergence_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Trains a perceptron based on the one vs. rest approach, i.e. train k classifiers to identify k classes\n",
    "    :param x: the observations array\n",
    "    :param y: the labels vector\n",
    "    :param d: is either the order of the polynomial kernel or the constant in the gaussian kernel\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param convergence_threshold: the threshold value upon we stop updating the perceptron, if the difference\n",
    "    in errors was smaller than that\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = x.shape[0] #number of examples\n",
    "    n = x.shape[1] #number of features\n",
    "    classes_num = 10 #number of classes \n",
    "     \n",
    "    error_per_epoch = []\n",
    "    y_arr = transform_y(y) \n",
    "    alpha = np.zeros((m,classes_num)) #Need to store alpha array at all iteration, as we need it to compute confidence\n",
    "    \n",
    "    K_train = calculate_kernel_single(x, d, kernel_choice)    \n",
    "    epochs = 0\n",
    "    while True:\n",
    "        alpha, errors = perceptron_epoch(x, y, y_arr, alpha, K_train)\n",
    "        \n",
    "        error_rate_current = error_per_epoch[-1] / x.shape[0] if epochs > 0 else 0\n",
    "        error_rate_next = errors[-1] / x.shape[0]\n",
    "\n",
    "        error_per_epoch.append(errors[-1])\n",
    "        if epochs > 0 and (error_rate_next > error_rate_current or \\\n",
    "            error_rate_current - error_rate_next < convergence_threshold):\n",
    "            break\n",
    "            \n",
    "        epochs += 1\n",
    "\n",
    "    return alpha, error_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_training_test_sets(data,r =1/5):\n",
    "    \"\"\"\n",
    "    Given a matrix M, of MxN dimensions, it splits it into (1 - r) and r sizes\n",
    "    for a training and test set respectively and separates it into X and Y,\n",
    "    assuming that the last column of the matrix denotes Y\n",
    "    :param data: the matrix to split\n",
    "    :param r: the ratio of test to train data\n",
    "    :return: the training set arrays X_train and y_train and\n",
    "          test set arrays X_test and y_test\n",
    "    \"\"\"\n",
    "    X = data[:,1:]\n",
    "    y = data[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=r, shuffle=True)    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_test(x_test,x_train,y_test, alphas, d, kernel_choice='Polynomial'):\n",
    "    \"\"\"\n",
    "    This function predicts the labels for a given set of observations.\n",
    "    :param x_test: the set of observations to predict\n",
    "    :param x_train: the observations in which the perceptron has been trained on\n",
    "    :param y_test: the true labels of x_test\n",
    "    :param alphas: the set of weights of the perceptron\n",
    "    :param d: is either the order of the polynomial kernel or the constant in the gaussian kernel\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the number of mistakes that the perceptron made, the predictions as well as the confidence values.\n",
    "    \"\"\"\n",
    "    # Calculate the Kernel matrix\n",
    "    K_test = calculate_kernel_double(x_train, x_test, d, kernel_choice)\n",
    "    \n",
    "    # Compute the confidence\n",
    "    confidence = (alphas.T @ K_test).T\n",
    "    \n",
    "    # Compute mistakes and predictions\n",
    "    preds = np.zeros(confidence.shape)\n",
    "    mistakes = 0\n",
    "    for i in range(y_test.shape[0]):\n",
    "        y_hat = confidence[i].argmax()\n",
    "        preds[i,y_hat] = 1\n",
    "        if y_hat != y_test[i]:\n",
    "            mistakes += 1\n",
    "            \n",
    "    return mistakes, preds, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now doing run  20 / 20  for d= 7 .........\r"
     ]
    }
   ],
   "source": [
    "def basic_results(d_arr, kernel_choice, runs):\n",
    "    \"\"\"\n",
    "    For every value in d_arr, and using the kernel specified in kernel_choice,\n",
    "    it performs \"runs\" iterations where it trains a perceptron based on randomly selected\n",
    "    80% of the data and tests on the rest 20%.\n",
    "    :param d_arr: an array of d values\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param runs: the number of runs to perform\n",
    "    :return: two arrays: one containing the errors recorded in the training set in every run and one for the test set.\n",
    "    \"\"\"\n",
    "    d_length = len(d_arr)\n",
    "    training_set_errors = np.zeros((d_length,runs))\n",
    "    test_set_errors = np.zeros((d_length,runs))\n",
    "    \n",
    "    for d in d_arr:\n",
    "        for i in range(runs):\n",
    "            print(\"Now doing run \", i+1, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "            X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "            alphas,_ = perceptron_train(X_train, y_train, d, kernel_choice=kernel_choice)\n",
    "\n",
    "            train_errors,_,_ = perceptron_test(X_train, X_train, y_train, alphas, d, kernel_choice=kernel_choice)\n",
    "            test_errors,_,_ = perceptron_test(X_test, X_train, y_test, alphas, d, kernel_choice=kernel_choice)\n",
    "\n",
    "            training_set_errors[d-1, i] = train_errors / y_train.shape[0]\n",
    "            test_set_errors[d-1, i] = test_errors / y_test.shape[0]\n",
    "    return training_set_errors, test_set_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:12:53.117431 d= 7 .........\n"
     ]
    }
   ],
   "source": [
    "d_arr = np.arange(1,8)\n",
    "runs = 20\n",
    "\n",
    "startTime = datetime.now()\n",
    "training_set_errors_pp_basic, test_set_errors_pp_basic = basic_results(d_arr, 'Polynomial', runs)\n",
    "time_pp_basic = datetime.now() - startTime\n",
    "print(\"\\nTime taken: \", time_pp_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0807 +- 0.0210</td>\n",
       "      <td>0.1003 +- 0.0229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0153 +- 0.0042</td>\n",
       "      <td>0.0470 +- 0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0074 +- 0.0031</td>\n",
       "      <td>0.0403 +- 0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0058 +- 0.0036</td>\n",
       "      <td>0.0358 +- 0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0032 +- 0.0016</td>\n",
       "      <td>0.0325 +- 0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0025 +- 0.0013</td>\n",
       "      <td>0.0331 +- 0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0074 +- 0.0235</td>\n",
       "      <td>0.0394 +- 0.0237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training set error rate Test set error rate\n",
       "1        0.0807 +- 0.0210    0.1003 +- 0.0229\n",
       "2        0.0153 +- 0.0042    0.0470 +- 0.0054\n",
       "3        0.0074 +- 0.0031    0.0403 +- 0.0052\n",
       "4        0.0058 +- 0.0036    0.0358 +- 0.0055\n",
       "5        0.0032 +- 0.0016    0.0325 +- 0.0043\n",
       "6        0.0025 +- 0.0013    0.0331 +- 0.0044\n",
       "7        0.0074 +- 0.0235    0.0394 +- 0.0237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_dataframe_error_rates(training_set_errors, test_set_errors, d_arr):\n",
    "    \"\"\"\n",
    "    Transformes the two training and test set error arrays into one, so as to be digested in a panda dataframe.\n",
    "    :param training_set_errors: an array containing the error recorded in the training set in every run.\n",
    "    :param test_set_errors: an array containing the error recorded in the test set in every run.\n",
    "    :param d_arr: an array of d values\n",
    "    :return: a zipped array containing the mean and standard deviation of errors in the training and test set for every\n",
    "    d value, as calculated upon all the runs.\n",
    "    \"\"\"\n",
    "    means_std = []\n",
    "    for d in range(len(d_arr)):\n",
    "        data_t = []\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(training_set_errors[d].mean(), \\\n",
    "                                                np.std(training_set_errors[d])))\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(test_set_errors[d].mean(), \\\n",
    "                                                np.std(test_set_errors[d])))\n",
    "        means_std.append(data_t)\n",
    "    return means_std\n",
    "    \n",
    "means_std = construct_dataframe_error_rates(training_set_errors, test_set_errors, d_arr)\n",
    "df = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having already allocated x_train, now perform cross validation on x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, kernel_choice, d, k):\n",
    "    \"\"\"\n",
    "    This function performs a k-fold cross validation on X, using a kernel of \"kernel_choice\" with parameter d.\n",
    "    :param X: the observations array\n",
    "    :param y: the labels vector\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param d: the parameter of the kernel\n",
    "    :param k: the number of splits, i.e. the k parameter in k-fold Cross Validation\n",
    "    :return: the mean of test error across the k runs of the CV process and its standard deviation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    MSE_cv_arr = np.zeros(k)\n",
    "    i = 0\n",
    "    \n",
    "    for train_index, cv_index in kf.split(X):\n",
    "        # Spit the matrix using the indices gained by the CV method and construct X and Y arrays\n",
    "        X_train = X[train_index]\n",
    "        X_cv = X[cv_index]\n",
    "        y_train = y[train_index]\n",
    "        y_cv = y[cv_index]\n",
    "    \n",
    "        # We are only interested in the alphas and not the MSE on the training set\n",
    "        alphas, errors = perceptron_train(X_train, y_train, d, kernel_choice = kernel_choice)\n",
    "        mistakes,_,_ = perceptron_test(X_cv, X_train, y_cv, alphas, d, kernel_choice = kernel_choice)\n",
    "        MSE_cv_arr[i] = mistakes / len(y_cv)\n",
    "        i += 1\n",
    "        \n",
    "    return MSE_cv_arr.mean(), np.std(MSE_cv_arr)\n",
    "\n",
    "def cv_process(d_arr, runs, kernel_choice, calculate_confusions):\n",
    "    \"\"\"\n",
    "    This function performs 5-fold cross validation, multiple times (specified by runs argument) across the different\n",
    "    values of d specified in d_arr using the kernel specified in kernel_choice\n",
    "    :param d_arr: an array of d values\n",
    "    :param runs: The number of runs to repeat the CV process\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param calculate_confusions: Whether or not to also calculate confusions on the test set\n",
    "    :return: the array of d_stars, the test_errors and the confusions found\n",
    "    \"\"\"\n",
    "    d_stars = np.zeros(runs)\n",
    "    test_errors = np.zeros(runs)\n",
    "    confusions = []\n",
    "    \n",
    "    if runs != 20:\n",
    "        print(\"WARNING: Change the number of runs to 20!!!\")\n",
    "        \n",
    "    for run in range(runs):\n",
    "        confusion = np.zeros((10, 10))\n",
    "\n",
    "        # In each run we will iterate through the d array and use all possible values of d\n",
    "        # Allocate 80/20 percent for training and test set\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "\n",
    "        CV_means = np.zeros(len(d_arr))\n",
    "        for i in range(len(d_arr)):\n",
    "            print(\"Now doing run \", run+1, \"/\", runs, \" for d=\", d_arr[i], \".........\", end='\\r')\n",
    "            MSE_CV_mean, _ = cross_validation(X_train, y_train, kernel_choice, d_arr[i], k=5)\n",
    "            CV_means[i] = MSE_CV_mean\n",
    "\n",
    "        # Train in whole 80% now with d_star\n",
    "        d_stars[run] = d_arr[CV_means.argmin()]\n",
    "        alphas, errors = perceptron_train(X_train, y_train, d_stars[run], kernel_choice = kernel_choice)\n",
    "\n",
    "        mistakes,preds_test,_ = perceptron_test(X_test, X_train, y_test, alphas, d_stars[run], kernel_choice = kernel_choice)\n",
    "        test_errors[run] = mistakes / len(y_test)\n",
    "                        \n",
    "        if calculate_confusions:\n",
    "            for i in range(X_test.shape[0]):\n",
    "                pred_label = preds_test[i].argmax()\n",
    "                if preds_test[i].argmax() != y_test[i]:\n",
    "                    confusion[int(y_test[i]), pred_label] += (1 / mistakes)\n",
    "            \n",
    "            confusions.append(confusion)\n",
    "            \n",
    "    return d_stars, test_errors, confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now doing run  20 / 20  for d= 7 .........\r"
     ]
    }
   ],
   "source": [
    "runs = 20\n",
    "startTime = datetime.now()\n",
    "d_stars_array, test_errors_array, confusions_array = cv_process(d_arr, runs, 'Polynomial', True)\n",
    "time_pp_cv = datetime.now() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:32:27.296995\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken: \", time_pp_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean d*:  5.4  with std:  0.8\n",
      "Mean test error:  0.03545698924731183  with std:  0.00511311239579829\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean d*: \", d_stars_array.mean(), \" with std: \", np.std(d_stars_array))\n",
    "print(\"Mean test error: \", test_errors_array.mean(), \" with std: \", np.std(test_errors_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.03 +- 0.04</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.03 +- 0.02</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.05 +- 0.02</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.03 +- 0.03</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.03 +- 0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.05 +- 0.04</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.03 +- 0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.04 +- 0.02</td>\n",
       "      <td>0.01 +- 0.02</td>\n",
       "      <td>0.03 +- 0.02</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.02 +- 0.02</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.01 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.03 +- 0.02</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "      <td>0.04 +- 0.03</td>\n",
       "      <td>0.00 +- 0.01</td>\n",
       "      <td>0.00 +- 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  0.00 +- 0.00  0.00 +- 0.00  0.02 +- 0.02  0.01 +- 0.02  0.01 +- 0.01   \n",
       "1  0.00 +- 0.00  0.00 +- 0.00  0.00 +- 0.01  0.00 +- 0.00  0.03 +- 0.04   \n",
       "2  0.01 +- 0.02  0.00 +- 0.01  0.00 +- 0.00  0.02 +- 0.02  0.02 +- 0.02   \n",
       "3  0.01 +- 0.01  0.00 +- 0.01  0.03 +- 0.02  0.00 +- 0.00  0.00 +- 0.00   \n",
       "4  0.00 +- 0.01  0.02 +- 0.02  0.01 +- 0.02  0.00 +- 0.00  0.00 +- 0.00   \n",
       "5  0.02 +- 0.02  0.00 +- 0.01  0.01 +- 0.01  0.05 +- 0.04  0.01 +- 0.01   \n",
       "6  0.02 +- 0.02  0.01 +- 0.01  0.01 +- 0.01  0.00 +- 0.00  0.01 +- 0.01   \n",
       "7  0.00 +- 0.00  0.01 +- 0.01  0.01 +- 0.02  0.00 +- 0.01  0.01 +- 0.02   \n",
       "8  0.01 +- 0.02  0.01 +- 0.02  0.01 +- 0.02  0.04 +- 0.02  0.01 +- 0.02   \n",
       "9  0.00 +- 0.01  0.01 +- 0.01  0.00 +- 0.01  0.00 +- 0.01  0.03 +- 0.02   \n",
       "\n",
       "              5             6             7             8             9  \n",
       "0  0.01 +- 0.01  0.01 +- 0.01  0.00 +- 0.01  0.01 +- 0.01  0.00 +- 0.01  \n",
       "1  0.00 +- 0.00  0.01 +- 0.01  0.00 +- 0.01  0.01 +- 0.02  0.00 +- 0.01  \n",
       "2  0.00 +- 0.01  0.01 +- 0.01  0.02 +- 0.02  0.01 +- 0.02  0.00 +- 0.01  \n",
       "3  0.05 +- 0.02  0.00 +- 0.00  0.01 +- 0.01  0.03 +- 0.03  0.00 +- 0.00  \n",
       "4  0.00 +- 0.01  0.01 +- 0.02  0.01 +- 0.02  0.01 +- 0.01  0.03 +- 0.03  \n",
       "5  0.00 +- 0.00  0.02 +- 0.02  0.00 +- 0.01  0.02 +- 0.02  0.01 +- 0.01  \n",
       "6  0.01 +- 0.01  0.00 +- 0.00  0.00 +- 0.00  0.00 +- 0.01  0.00 +- 0.01  \n",
       "7  0.00 +- 0.00  0.00 +- 0.00  0.00 +- 0.00  0.01 +- 0.01  0.03 +- 0.02  \n",
       "8  0.03 +- 0.02  0.01 +- 0.01  0.02 +- 0.02  0.00 +- 0.00  0.01 +- 0.01  \n",
       "9  0.00 +- 0.01  0.00 +- 0.00  0.04 +- 0.03  0.00 +- 0.01  0.00 +- 0.00  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusions_matrix = []\n",
    "for i in range(10):\n",
    "    confusions_i = []\n",
    "    for j in range(10):\n",
    "        confusions_ij = np.asarray([confusions_array[r][i,j] for r in range(runs)])\n",
    "        confusions_i.append(\"{0:.2f} +- {1:.2f}\".format(confusions_ij.mean(), np.std(confusions_ij)))\n",
    "    confusions_matrix.append(confusions_i)\n",
    "    \n",
    "df = pd.DataFrame(data=confusions_matrix)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 - Hardest numbers to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hardest_images(indices):\n",
    "    \"\"\"\n",
    "    This helper function plots images in an 1x(len(indices)) grid of plots.\n",
    "    :param indices: The indices of examples to plot (from array 'x')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(30,30))\n",
    "    k = 1\n",
    "    for i in indices:\n",
    "        a1 = plt.subplot(1, len(indices), k)\n",
    "        pixels = np.array(x[i], dtype='uint8')\n",
    "        pixels = pixels.reshape((16, 16))\n",
    "        plt.title(\"Real: {0}\".format(y[i]))\n",
    "        a1.imshow(pixels, cmap='gray')\n",
    "        k+=1\n",
    "        \n",
    "def find_hardest_elements(x, y, d_star, runs, k_splits=5):\n",
    "    \"\"\"\n",
    "    This function performs a number of runs over the data set in order to find the 5 images which are the most\n",
    "    difficult to predict. The way this process is done is the following:\n",
    "        - We perform a 5-fold split over the data set, dividing into 80% for training and 20% for testing\n",
    "        - We use the mean d_star as calculated over 20 runs of CV in the data set, to train a perceptron on the 80%\n",
    "          of the data set.\n",
    "        - We predict the remaining 20% and we store the confidences we obtain in each split and run\n",
    "    At the end of the process, we sort the confidences in ascending order and return the 5 first indices\n",
    "    (i.e. the observations which were harder to predict)\n",
    "    :param x: the observations array\n",
    "    :param y: the labels vector\n",
    "    :param d_star: the value of d to use while training the perceptron\n",
    "    :param runs: the number of runs to perform\n",
    "    :param k_splits: the number of splits to perform while generating the training/test sets.\n",
    "    :return: the indices of the elements of x, that we predict the least confidence with\n",
    "    \"\"\"\n",
    "    least_confidences = np.zeros(data.shape[0])\n",
    "    \n",
    "    for run in range(runs):\n",
    "        print(\"Finding hardest elements, run \", run+1, \"/\", runs,\".....\", end='\\r')\n",
    "        \n",
    "        # Split into 80%/20% for training and test set\n",
    "        kf = KFold(n_splits=k_splits, shuffle=True)\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            X_train = x[train_index]\n",
    "            X_test = x[test_index]\n",
    "            y_train = y[train_index]\n",
    "            y_test = y[test_index]\n",
    "\n",
    "            alphas,_ = perceptron_train(X_train, y_train, d_star)\n",
    "            _, preds_test, confidences = perceptron_test(X_test, X_train, y_test, alphas, d_star)\n",
    "            \n",
    "            for i in range(X_test.shape[0]):\n",
    "                pred_label = preds_test[i].argmax()\n",
    "                least_confidences[test_index[i]] += confidences[i][int(y_test[i])] / 5\n",
    "    \n",
    "    least_confidences /= runs\n",
    "\n",
    "    conf_indices = np.argsort(least_confidences)[:5]\n",
    "    return conf_indices, np.sort(least_confidences)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding hardest elements, run  20 / 20 .....\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABq8AAAFQCAYAAAAhlOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X+M7Xld3/HX2x1RFpCF7hV1F1ykFIOEX3ujKIka1h+roktK/4CohUK7aVIECQkFf7ZpNKQSQaPVrIAXI11bEQshiqwIJW2BeC/ya1kUiuuyCOwg8kOwrui7f8xZmL07d36d8z3nM+c8HsnNnTlz7jnve2bmOd877/udU90dAAAAAAAAGMGXrHoAAAAAAAAAuIPlFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLK4ZXVW+qqn+96jkA1pXOAkxLZwGmp7UA09JZls3yioWoqpur6m+r6m+q6qNVdaaq7rmCOX51NsMdv/6uqj6zz/UfWVXnqupzs98fucx5AQ5LZwGmNUpnZ7M8ezbDp6vqZVX1Zftc96qqet+ss2+sqq9d5qwARzFKa6vqy6rqRVX1l1X111X1X6rqS/e5vmNa4ETQWdaJ5RWL9P3dfc8kj0zyqCTPX/YA3f1vu/ued/xKcn2S397rulV1tySvTvKbSe6T5OVJXj27HGBEOgswrZV3tqq+O8nzklyV5GuTfF2S/3iB616a5FVJfjLJfZOcTfLfljMpwLGtvLXZ6ezpJA9L8s+SPDrJT+x1Rce0wAmks6wFyysWrrs/muQPshPIJF/Ytr+wqm6pqo/N/uf+3Wdvu09Vvbaqtmeb+NdW1eXzzlFV90jyxOwEby/fnmQryYu7+++6+xeTVJLHzXvfAFPSWYBprbizT0ny0u6+sbv/Osl/SvLUC1z3nye5sbt/u7v/X5L/kOQRVfX1x7xvgKVZcWu/P8kvdvcnuns7yS8medoFrvvtcUwLnEA6y0lnecXCzaL2PUk+sOviF2Rny/7IJP80yWVJfmr2ti9J8uvZ+Z+lD0jyt0l+6QK3/YCq+mRVPeAQozwxyXaSN1/g7d+Q5F3d3bsue9fscoBh6SzAtFbc2W9I8s5dr78zyf2q6p8cdN3u/myS/xudBU6AAY5p67yXL6+qe+9xPce0wImks5x0llcs0v+onec9+VCS25L8dJJUVSW5NsmzZ9v2zyT52SRPSpLu/qvu/p3u/tzsbT+T5Nv2uoPuvqW7L+nuWw4xz1OS/MZ54dvtnkk+dd5ln0pyr0PcNsAq6CzAtEbo7PntvOPlvdqps8BJNEJrX5fkWVV1qqq+KskzZ5dfvMd1tRY4aXSWtbC16gFYK0/o7j+sqm9L8l+TXJrkk0lOZSdM53YamWRn235RklTVxUlelOTq7Pxc0yS5V1Vd1N3/cJxBZlv/b0/yb/a52t8k+YrzLvuKJJ85zn0CLIHOAkxrhM6e3847Xt6rnToLnEQjtPZnklyS5B1J/i7Jr2XneWE+tsd1tRY4aXSWteDMKxauu/9nkjNJXji76OPZOc30G2Yb+Uu6+96zJw5MkuckeUiSb+rur0jyrbPLd59aelQ/nOR/d/cH97nOjUkeXrtqneThs8sBhqWzANNacWdvTPKIXa8/IsnHuvuvDrru7LkIHxSdBU6AVba2u/+2u5/R3Zd199cl+ask57r7H/e4umNa4ETSWU46yyum8uIk31lVj5hF6deSvKiqvjJJquqyqvru2XXvlZ1wfrKq7pvZqaxz+pfZifN+3pTkH5I8c/Zkhc+YXf5HC7h/gKnpLMC0VtXZ30jy9Kp6aFVdkuQncuHe/m6Sh1XVE6vqy7PzfAXv6u73zXH/AMu0ktbObvdrasdjkvzkPrf3pjimBU4uneXEsrxiEt29nZ1/eN/xhH//PjtPDvjWqvp0kj/MziY/2Yno3bOz/X9rdn4m6p5mTwb4N/s9GWBVfXOSy5P89h5v+/2q+rHZjLcneUJ2vgH7ySRPy85ptbcf4a8KsBI6CzCtVXW2u1+X5D8neWOSW5L8RXb9Q7+qbqyqH9w14xOz82NZ/jrJN2X2nAUAJ8EKj2kflOT/JPlskpcneV53v37Xn3dMC6wFneUkqws/xzoAAAAAAAAslzOvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYWwt884uvfTSvuKKK5Z5lwBfcO7cuY9396lVzzElnWUe586dW/UIk7nyyitXPcJG2ITOJloLrM7NN9+cj3/847XqOaY2WmfX9RjJ8RHsbROOaauqVz0DsNEO1dmlLq+uuOKKnD17dpl3CfAFVfUXq55hajrLPKrW93thPi+WYxM6m2gtsDqnT59e9QhLMVpn1/UYaaTHGEayKce0XNiiut9tR3jSrOvX/AE/Fg/VWT82EAAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGMdfyqqqurqo/raoPVNXzFjUUAF+ktQDT0lmAaekswPS0Flg3x15eVdVFSX45yfckeWiSJ1fVQxc1GABaCzA1nQWYls4CTE9rgXU0z5lX35jkA939we6+PclvJblmMWMBMKO1ANPSWYBp6SzA9LQWWDvzLK8uS/KhXa/fOrsMgMXRWoBp6SzAtHQWYHpaC6yduZ7z6jCq6tqqOltVZ7e3t6e+O4CNo7MA09NagGnpLMC0dnd21bMAHMY8y6sPJ7n/rtcvn112J919XXef7u7Tp06dmuPuADbSga3VWYC5OKYFmJbOAkzvSN87WOpkAMc0z/Lqj5M8uKoeWFV3S/KkJK9ZzFgAzGgtwLR0FmBaOgswPa0F1s7Wcf9gd3++qp6R5A+SXJTkZd1948ImA0BrASamswDT0lmA6WktsI6OvbxKku7+vSS/t6BZANiD1gJMS2cBpqWzANPTWmDdzPNjAwEAAAAAAGChLK8AAAAAAAAYhuUVAAAAAAAAw7C8AgAAAAAAYBhbqx4AAFg/VbWQ2+nuhdwOAMBxLep4ZFHHR4vieO1go73PRrPO7/t1d+WVV+bs2bNz3866fo6s68f2Ond/XT8WN50zrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAwLK8AAAAAAAAYxtaqBwAA5lNVqx4BAIADdPdCbme0Y79FzbOuj886G+19z/J53+1vtB6N+P5a146M9r5flGV/rXbmFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYRx7eVVV96+qN1bVe6vqxqp61iIHA0BrAaamswDT0lmA6WktsI625vizn0/ynO5+e1XdK8m5qrqhu9+7oNkA0FqAqekswLR0FmB6WgusnWOfedXdH+nut89e/kySm5JctqjBANBagKnpLMC0dBZgeloLrKOFPOdVVV2R5FFJ3raI2wPgrrQWYFo6CzAtnQWYntYC62Lu5VVV3TPJ7yT50e7+9B5vv7aqzlbV2e3t7XnvDmAj7ddanQWYn2NagGnpLMD0fO8AWCdzLa+q6kuzE8RXdPer9rpOd1/X3ae7+/SpU6fmuTuAjXRQa3UWYD6OaQGmpbMA0/O9A2DdHHt5VVWV5KVJburun1/cSADcQWsBpqWzANPSWYDpaS2wjuY58+qxSX44yeOq6h2zX9+7oLkA2KG1ANPSWYBp6SzA9LQWWDtbx/2D3f2/ktQCZwHgPFoLMC2dBZiWzgJMT2uBdTTXc14BAAAAAADAIlleAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAwtlY9AACcNFXr+Ty4i/x7dffCbgsAgC9a1HHWaMe0o82zSKMdG6/zYw2cLKP1cVHW9e+1bM68AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYW6seAACWpapWPcKdjDbPiLp7IbezqMd6UfMAADA+x36wmdb13+ojNs1jzX6ceQUAAAAAAMAwLK8AAAAAAAAYhuUVAAAAAAAAw7C8AgAAAAAAYBhzL6+q6qKq+pOqeu0iBgLgznQWYHpaCzAtnQWYntYC62QRZ149K8lNC7gdAPamswDT01qAaekswPS0Flgbcy2vquryJN+X5CWLGQeA3XQWYHpaCzAtnQWYntYC62beM69enOS5Sf5xAbMAcFc6CzA9rQWYls4CTE9rgbVy7OVVVT0+yW3dfe6A611bVWer6uz29vZx7w5g4+gswPS0FmBaOgswvcO0VmeBk2aeM68em+QHqurmJL+V5HFV9ZvnX6m7r+vu0919+tSpU3PcHcDG0VmA6WktwLR0FmB6B7ZWZ4GT5tjLq+5+fndf3t1XJHlSkj/q7h9a2GQAG05nAaantQDT0lmA6WktsI7mfc4rAAAAAAAAWJitRdxId78pyZsWcVsA3JXOAkxPawGmpbMA09NaYF048woAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDC2Vj0AABykqlY9wp2MNs86G+2xXtQ83b2Q24FkvM+TRVnU58m6Pj6JlizLOn8MwToasY06AgBH58wrAAAAAAAAhmF5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIaxteoB1kFVrXqEjdDdqx4BOOEW1evReuTrEJxMPnf35/E5mMcI4K7WuY3r+u8ZGM2iPkdG65GGcNI48woAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDDmWl5V1SVV9cqqel9V3VRV37yowQDYobUA09JZgGnpLMD0tBZYN1tz/vlfSPK67v4XVXW3JBcvYCYA7kxrAaalswDT0lmA6WktsFaOvbyqqnsn+dYkT02S7r49ye2LGQuARGsBpqazANPSWYDpaS2wjub5sYEPTLKd5Ner6k+q6iVVdY8FzQXADq0FmJbOAkxLZwGmp7XA2plnebWV5NFJfqW7H5Xks0med/6VquraqjpbVWe3t7fnuDuAjXRga3UWYC6OaQGmpbMA0/O9A2DtzLO8ujXJrd39ttnrr8xOJO+ku6/r7tPdffrUqVNz3B3ARjqwtToLMBfHtADT0lmA6fneAbB2jr286u6PJvlQVT1kdtFVSd67kKkASKK1AFPTWYBp6SzA9LQWWEdbc/75H0nyiqq6W5IPJvlX848EwHm0FmBaOgswLZ0FmJ7WAmtlruVVd78jyekFzQLAHrQWYFo6CzAtnQWYntYC62ae57wCAAAAAACAhbK8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhbK16gOOoqlWPwAp4vwPz6u5Vj8ARLep95msIi3Tu3LmhPqZGmgVG4us+sG5G69po8wDLMeK/P0abaVHzbHpnnXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAwLK8AAAAAAAAYhuUVAAAAAAAAw7C8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDC2lnln586dS1Ut8y7hRFjU50V3L+R2YDSjfWyP9rVsUY/PaH+vZMyZYDTr3AA202hf90ea5/Tp06seAVgDI3UNRuS4+OTRtfXkzCsAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYcy2vqurZVXVjVb2nqq6vqi9f1GAA7NBagGnpLMC0dBZgeloLrJtjL6+q6rIkz0xyursfluSiJE9a1GAAaC3A1HQWYFo6CzA9rQXW0bw/NnAryd2raivJxUn+cv6RADiP1gJMS2cBpqWzANPTWmCtHHt51d0fTvLCJLck+UiST3X36xc1GABaCzA1nQWYls4CTE9rgXU0z48NvE+Sa5I8MMnXJLlHVf3QHte7tqrOVtXZ448JsJkO09rdnd3e3l7FmAAnlmNagGkdp7OOaQGOxvcOgHU0z48N/I4kf97d293990leleRbzr9Sd1/X3ae7+/Qc9wWwqQ5s7e7Onjp1aiVDApxgjmkBpnXkzjqmBTgy3zsA1s48y6tbkjymqi6uqkpyVZKbFjMWADNaCzAtnQWYls4CTE9rgbUzz3NevS3JK5O8Pcm7Z7d13YLmAiBaCzA1nQWYls4CTE9rgXW0Nc8f7u6fTvLTC5oFgD1oLcC0dBZgWjoLMD2tBdbNPD82EAAAAAAAABbK8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhrG1zDu78sorc/bs2WXe5b6qatUjQJKku1c9wp2MNs+i+Jw/udb1fbeuf69kff9u69pHjmZdj2nX9eN7XXs0otE+hkabB0bjc+Rgo30N8T4D5rGohozWRtaXM68AAAAAAAAYhuUVAAAAAAAAw7C8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGMbWqgdYpe5e9QgALFFVrXqEjeFrLCzPun6+LarZ6/r4rDPvM2AejvmBUYx2PKuPnDTOvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMIwDl1dV9bKquq2q3rPrsvtW1Q1V9f7Z7/eZdkyA9aa1ANPSWYBp6SzA9LQW2CSHOfPqTJKrz7vseUne0N0PTvKG2esAHN+ZaC3AlM5EZwGmdCY6CzC1M9FaYEMcuLzq7jcn+cR5F1+T5OWzl1+e5AkLngtgo2gtwLR0FmBaOgswPa0FNslxn/Pqft39kdnLH01yvwXNA8AXaS3AtHQWYFo6CzA9rQXW0nGXV1/Q3Z2kL/T2qrq2qs5W1dnt7e157w5gI+3XWp0FmJ9jWoBp6SzA9HzvAFgnx11efayqvjpJZr/fdqErdvd13X26u0+fOnXqmHcHsJEO1VqdBTg2x7QA09JZgOn53gGwlo67vHpNkqfMXn5KklcvZhwAdtFagGnpLMC0dBZgeloLrKUDl1dVdX2StyR5SFXdWlVPT/KCJN9ZVe9P8h2z1wE4Jq0FmJbOAkxLZwGmp7XAJtk66Ard/eQLvOmqBc8CsLG0FmBaOgswLZ0FmJ7WApvkuD82EAAAAAAAABbO8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhrG16gEAYFm6e9UjDK2qVj3CXXifweby+Q+wWUY8Fl0UX9NgM432ub+ozo7291pnm/4+c+YVAAAAAAAAw7C8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMPYWvUAAHCQ7l7I7VTVQm5nUdb17wUAAACwLIv6vsiivk/DYjjzCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMA5cXlXVy6rqtqp6z67Lfq6q3ldV76qq362qS6YdE2C9aS3AtHQWYFo6CzA9rQU2yWHOvDqT5OrzLrshycO6++FJ/izJ8xc8F8CmOROtBZjSmegswJTORGcBpnYmWgtsiAOXV9395iSfOO+y13f352evvjXJ5RPMBrAxtBZgWjoLMC2dBZie1gKbZBHPefW0JL+/gNsB4MK0FmBaOgswLZ0FmJ7WAmtjruVVVf14ks8necU+17m2qs5W1dnt7e157g5gIx3UWp0FmI9jWoBp6SzA9HzvAFg3x15eVdVTkzw+yQ92d1/oet19XXef7u7Tp06dOu7dAWykw7RWZwGOzzEtwLR0FmB6vncArKOt4/yhqro6yXOTfFt3f26xIwGQaC3A1HQWYFo6CzA9rQXW1YFnXlXV9UnekuQhVXVrVT09yS8luVeSG6rqHVX1qxPPCbDWtBZgWjoLMC2dBZie1gKb5MAzr7r7yXtc/NIJZgHYWFoLMC2dBZiWzgJMT2uBTXLs57wCAAAAAACARbO8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhbK16AABYlu5e9QgcUVUt5Ha87wEAOCrHkLCZFvW5v6h/z/p3MZvKmVcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAwLK8AAAAAAAAYhuUVAAAAAAAAw7C8AgAAAAAAYBiWVwAAAAAAAAzD8goAAAAAAIBhWF4BAAAAAAAwDMsrAAAAAAAAhmF5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADGNr1QMAAOunqhZyO929kNsBAGAaizruAwDubNO/J+LMKwAAAAAAAIZheQUAAAAAAMAwLK8AAAAAAAAYhuUVAAAAAAAAwzhweVVVL6uq26rqPXu87TlV1VV16TTjAWwGrQWYls4CTEtnAaantcAmOcyZV2eSXH3+hVV1/yTfleSWBc8EsInORGsBpnQmOgswpTPRWYCpnYnWAhviwOVVd785ySf2eNOLkjw3SS96KIBNo7UA09JZgGnpLMD0tBbYJMd6zququibJh7v7nQueB4AZrQWYls4CTEtnAaantcC62jrqH6iqi5P8WHZORT3M9a9Ncm2SPOABDzjq3QFspKO0VmcBjs4xLcC0dBZger53AKyz45x59aAkD0zyzqq6OcnlSd5eVV+115W7+7ruPt3dp0+dOnX8SQE2y6Fbq7MAx+KYFmBaOgswPd87ANbWkc+86u53J/nKO16fhfF0d398gXMBbDStBZiWzgJMS2cBpqe1wDo78Myrqro+yVuSPKSqbq2qp08/FsBm0VqAaekswLR0FmB6WgtskgPPvOruJx/w9isWNg3AhtJagGnpLMC0dBZgeloLbJLjPOcVAAAAAAAATMLyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGsbXqAQCAMXT3qkcAAIC5OKYF1s1oXVvkPFW1kNsZ7TFiMZx5BQAAAAAAwDAsrwAAAAAAABiG5RUAAAAAAADDsLwCAAAAAABgGJZXAAAAAAAADMPyCgAAAAAAgGFYXgEAAAAAADAMyysAAAAAAACGYXkFAAAAAADAMCyvAAAAAAAAGIblFQAAAAAAAMOwvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAwqruXd2dV20n+4oCrXZrk40sY57DMsz/z7M88B1vmTF/b3aeWdF8robMLMdo8yXgzmWd/mzzP2nc20doFMc/+zLO/TZ5HZ79okz8ODsM8+zPP/kabJ9HahdLZhRltJvPszzz7G66zS11eHUZVne3u06ue4w7m2Z959meeg40407ob7TE3z8FGm8k8+zMPyXiPu3n2Z579mWd/o82zKUZ73M2zP/PszzwHG3GmdTfaYz7aPMl4M5lnf+bZ32jzJH5sIAAAAAAAAAOxvAIAAAAAAGAYIy6vrlv1AOcxz/7Msz/zHGzEmdbdaI+5eQ422kzm2Z95SMZ73M2zP/Pszzz7G22eTTHa426e/Zlnf+Y52IgzrbvRHvPR5knGm8k8+zPP/kabZ7znvAIAAAAAAGBzjXjmFQAAAAAAABtqmOVVVV1dVX9aVR+oqucNMM/9q+qNVfXeqrqxqp41wEwXVdWfVNVrVz1LklTVJVX1yqp6X1XdVFXfvOJ5nj17X72nqq6vqi9f8v2/rKpuq6r37LrsvlV1Q1W9f/b7fVY8z8/N3l/vqqrfrapLVjnPrrc9p6q6qi5d1jybaqTWjtjZZKzW6uyeM2jtEefZ9TatXQKdPZjO7juPzh5uHp3dcFp74EzDdDbR2j3uX2ePOM+ut+nskujswUZqrc7uOYPWHnGeXW8bprVDLK+q6qIkv5zke5I8NMmTq+qhq50qn0/ynO5+aJLHJPl3A8z0rCQ3rXiG3X4hyeu6++uTPCIrnK2qLkvyzCSnu/thSS5K8qQlj3EmydXnXfa8JG/o7gcnecPs9VXOc0OSh3X3w5P8WZLnr3ieVNX9k3xXkluWOMtGGrC1I3Y2Gau1OntXZ6K1R51Ha5dEZw9NZ/egs0eaR2c3mNYeykidTbT2fGeis0edR2eXSGcPbaTW6uxdnYnWHnWe4Vo7xPIqyTcm+UB3f7C7b0/yW0muWeVA3f2R7n777OXPZOeT/rJVzVNVlyf5viQvWdUMu1XVvZN8a5KXJkl3397dn1ztVNlKcveq2kpycZK/XOadd/ebk3zivIuvSfLy2csvT/KEVc7T3a/v7s/PXn1rkstXOc/Mi5I8N4kn4JveUK0drbPJWK3V2b1p7dHnmdHa5dDZA+jsgXT2EPNMhfkWAAAD2UlEQVTo7MbT2n2M1NlEa/eis0efZ0Znl0dnDzBSa3V2b1p79HlmhmrtKMury5J8aNfrt2bFEdqtqq5I8qgkb1vhGC/OzgfOP65wht0emGQ7ya/PTpF9SVXdY1XDdPeHk7wwO1vhjyT5VHe/flXz7HK/7v7I7OWPJrnfKoc5z9OS/P4qB6iqa5J8uLvfuco5NsiwrR2ks8lYrdXZw9PafWjtUunswXT2AnT22HR282jt/kbqbKK1h6Wz+9DZpdPZg43UWp09PK3dx4itHWV5NayqumeS30nyo9396RXN8Pgkt3X3uVXc/wVsJXl0kl/p7kcl+WyWe6rlncx+Ruk12Qn21yS5R1X90Krm2Ut3dwbZWlfVj2fntOtXrHCGi5P8WJKfWtUMjGGEzs7mGK21OnsMWnuXGbQWnb0wnT0Gnb3LDDpLkjFaO2BnE609Mp29yww6S5IxOjubY7TW6uwxaO1dZhiytaMsrz6c5P67Xr98dtlKVdWXZieKr+juV61wlMcm+YGqujk7p+o+rqp+c4XzJDv/6+LW7r7jfzq8MjuhXJXvSPLn3b3d3X+f5FVJvmWF89zhY1X11Uky+/22Fc+Tqnpqkscn+cFZqFflQdn5QvbO2cf25UneXlVftcKZ1t1wrR2os8l4rdXZw9PaC9Pa5dLZ/ens/nT2CHR2o2nthY3W2URrD0tnL0xnl09n9zdaa3X28LT2woZs7SjLqz9O8uCqemBV3S07T+L2mlUOVFWVnZ8VelN3//wqZ+nu53f35d19RXYemz/q7pVurLv7o0k+VFUPmV10VZL3rnCkW5I8pqounr3vrsoYT5r4miRPmb38lCSvXuEsqaqrs3Na8w909+dWOUt3v7u7v7K7r5h9bN+a5NGzjy2mMVRrR+psMl5rdfZItPYCtHbpdHYfOnsgnT0knd14WnsBo3V2NpPWHo7OXoDOroTO7mO01urskWjtBYza2iGWV73zxGTPSPIH2flg/u/dfeNqp8pjk/xwdrbn75j9+t4VzzSaH0nyiqp6V5JHJvnZVQ0y+98Fr0zy9iTvzs7H9nXLnKGqrk/yliQPqapbq+rpSV6Q5Dur6v3Z+Z8HL1jxPL+U5F5Jbph9TP/qiudhiQZsrc4eTGfPo7XHmocl0dkTSWfPo7PHmocl0toTSWt30dljzcMS6eyJpLPn0dpjzTOcWu3ZaAAAAAAAAPBFQ5x5BQAAAAAAAInlFQAAAAAAAAOxvAIAAAAAAGAYllcAAAAAAAAMw/IKAAAAAACAYVheAQAAAAAAMAzLKwAAAAAAAIZheQUAAAAAAMAw/j9/yPEegs20KgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2160x2160 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_indices, least_confidences = find_hardest_elements(x, y, int(d_stars.mean()), runs=20)\n",
    "plot_hardest_images(conf_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 - Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:28:48.390727 d= 7 .........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0621 +- 0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0625 +- 0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000 +- 0.0001</td>\n",
       "      <td>0.0624 +- 0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0633 +- 0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0649 +- 0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0746 +- 0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001 +- 0.0001</td>\n",
       "      <td>0.0967 +- 0.0066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training set error rate Test set error rate\n",
       "1        0.0000 +- 0.0000    0.0621 +- 0.0057\n",
       "2        0.0000 +- 0.0000    0.0625 +- 0.0042\n",
       "3        0.0000 +- 0.0001    0.0624 +- 0.0052\n",
       "4        0.0000 +- 0.0000    0.0633 +- 0.0063\n",
       "5        0.0000 +- 0.0000    0.0649 +- 0.0051\n",
       "6        0.0000 +- 0.0000    0.0746 +- 0.0044\n",
       "7        0.0001 +- 0.0001    0.0967 +- 0.0066"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = 20\n",
    "d_arr = np.arange(1,8)\n",
    "\n",
    "startTime = datetime.now()\n",
    "training_set_errors_gaussian, test_set_errors_gaussian = basic_results(d_arr, 'Gaussian', runs)\n",
    "time_pg_cv = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_pg_cv)\n",
    "\n",
    "means_std = construct_dataframe_error_rates(training_set_errors_gaussian, test_set_errors_gaussian, d_arr)\n",
    "df = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  1:01:32.725772 d= 7 .........\n",
      "Mean d*:  2.9  with std:  1.1357816691600546\n",
      "Mean test error:  0.06196236559139785  with std:  0.00670832242659558\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation with Gaussian Kernel, find d_star\n",
    "runs = 20\n",
    "startTime = datetime.now()\n",
    "d_stars_gaussian2, test_errors_gaussian2, confusions2 = cv_process(d_arr, runs, 'Gaussian', False)\n",
    "time_pg_cv2 = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_pg_cv2)\n",
    "print(\"Mean d*: \", d_stars_gaussian2.mean(), \" with std: \", np.std(d_stars_gaussian2))\n",
    "print(\"Mean test error: \", test_errors_gaussian2.mean(), \" with std: \", np.std(test_errors_gaussian2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 - Choose an alternative method to generalise k-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One vs. one approach: train k*(k-1)/2 binary classifiers to identify k classes\n",
    "#For example, one classifier could be trained to distinguish between digit 0 and digit 1.\n",
    "#Consider symmetry when computing prediction\n",
    "\n",
    "# Write a function that trains a binary classifier given two classes, given kernel:\n",
    "def classifier_ovo(class1,class2,K,alpha_ovo,iter_num):\n",
    "    \"\"\"\n",
    "    Given a kernel K and a set of weights, this function returns the signs denoting the votes of the one-versus-one\n",
    "    classifier\n",
    "    :param K: the kernel matrix K\n",
    "    :param alpha_ovo: the set of weights for the one-versus-one classifiers\n",
    "    :param iter_num: the number of iteration we are in\n",
    "    :return: a vote, within (-1,1)\n",
    "    \"\"\"\n",
    "    vote = np.sign(((alpha_ovo[:].T @K[iter_num,:]).T))\n",
    "    return vote #returns a vote, within (-1,1)\n",
    "\n",
    "def perceptron_train_ovo(x,y,d=2,kernel_choice='Polynomial',max_epoch=10, tol=0.01):\n",
    "    \"\"\"\n",
    "    This function trains the classifiers needed for the one-versus-one approach of perceptron\n",
    "    :param x: the observations array\n",
    "    :param y: the labels vector\n",
    "    :param d: the value of d used in the calculation of the kernel matrix\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param max_epoch: the maximum number of epochs to perform while training the perceptrons\n",
    "    :param tol: a threshold value which denotes the minimum change in error rate that we should have\n",
    "    in order to continue training\n",
    "    :return: a set of weights alpha (matrix of (m X n X n), where m is the number of examples,\n",
    "    n is the number of classes) and the number of errors in the last epoch\n",
    "    \"\"\"\n",
    "    m = x.shape[0] #number of examples\n",
    "    n = x.shape[1] #number of features\n",
    "    classes_num = 10 #number of classes \n",
    "    error_per_epoch = np.zeros(max_epoch)\n",
    "    errors = np.zeros(m)\n",
    "    K_train = calculate_kernel_single(x, d, kernel_choice)\n",
    "    \n",
    "    num_errors = 0 \n",
    "    alpha = np.zeros((m,classes_num,classes_num)) \n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        errors = np.zeros(m)\n",
    "        num_errors = 0 #This should be bounded..? Maybe calculate the bound in the explanation\n",
    "    \n",
    "        #iterate through training set\n",
    "        for t in range(m):\n",
    "            if t<1:\n",
    "                alpha_prev = alpha[0,:,:] #when t=0, the previous alpha is set to be 0\n",
    "            else:\n",
    "                alpha_prev = alpha[t-1,:,:] #\n",
    "\n",
    "            x_t = x[t,:]\n",
    "            y_t = y[t]\n",
    "\n",
    "            votes_board = np.zeros((classes_num, classes_num)) #zero on the horizontal. \n",
    "            classes_list = np.array(range(classes_num))\n",
    "\n",
    "            for i in range(classes_num):\n",
    "                c1 = classes_list[i]\n",
    "                classes_rest = classes_list[classes_list>c1]\n",
    "                for j in range(len(classes_rest)):\n",
    "                    c2 = classes_rest[j]\n",
    "                    alpha_ovo = alpha[:,c1,c2]\n",
    "                    vote = classifier_ovo(c1,c2,K_train,alpha_ovo,iter_num=t)\n",
    "                    votes_board[c1,c2] = vote\n",
    "\n",
    "            #Count the votes in the board\n",
    "            votes_count = votes_board.sum(axis=0)\n",
    "            pred_t = votes_count.argmax()\n",
    "\n",
    "            if pred_t!=y_t:\n",
    "                num_errors +=1\n",
    "\n",
    "                #increase alpha for all the positive classifier of the correct label.\n",
    "                #decrease alpha for the negative classifier of the false label. \n",
    "                alpha_t = alpha_prev #initialize it to its previous form\n",
    "                alpha_t[:,int(y_t)] =+1 # column belonging to correct label class +=1\n",
    "                alpha_t[:,int(pred_t)] =-1 # column belonging to false predicted class -=1\n",
    "\n",
    "                #store alpha_t into the matrix for future reference\n",
    "                alpha[t,:,:] = alpha_t\n",
    "            \n",
    "            errors[t] = num_errors \n",
    "        \n",
    "        error_per_epoch[epoch] = errors[-1]\n",
    "        \n",
    "        if epoch>1:\n",
    "            diff_rates = (error_per_epoch[epoch-1] - error_per_epoch[epoch])/m\n",
    "            \n",
    "            #Stop if the error rate has increased, \n",
    "            #or the difference in error rate between the previous one and the current one < tolerance. \n",
    "            if diff_rates<tol or diff_rates<0:\n",
    "                print('difference in error rate', diff_rates)\n",
    "                print('break point at epoch=', epoch )\n",
    "                break\n",
    "        \n",
    "    return alpha, error_per_epoch[:epoch+1]\n",
    "\n",
    "def perceptron_test_ovo(x_test,x_train,y_test,alphas, d,kernel_choice='Polynomial'):\n",
    "    \"\"\"\n",
    "    This function predicts the label values of the examples found in x_test array using the one-versus-one perceptrons\n",
    "    approach.\n",
    "    :param x_test: a set of unseen examples that we wish to get predictions on\n",
    "    :param x_train: the set of observations that the perceptrons have been trained on\n",
    "    :param y_test: the true labels of x_test, used for calculating error rates\n",
    "    :param alphas: the set of weights used for the perceptrons\n",
    "    :param d: the value of d used in the calculation of the kernel matrix\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the number of mistakes done on the test set as well as the actual predictions\n",
    "    \"\"\"\n",
    "    m_test = x_test.shape[0]\n",
    "    m_train = x_train.shape[0]\n",
    "    \n",
    "    K_test = calculate_kernel_double(x_train, x_test, d, kernel_choice)\n",
    "    \n",
    "    classes_num = 10\n",
    "    classes_list = np.array(range(classes_num))\n",
    "    votes_ovo =np.zeros((m_test,10,10))\n",
    "    \n",
    "    for i in range(classes_num):\n",
    "        c1 = classes_list[i]\n",
    "        classes_rest = classes_list[classes_list>c1]\n",
    "        for j in range(len(classes_rest)):\n",
    "            c2 = classes_rest[j]\n",
    "            alphas_ovo_c1c2 = alphas[:,int(c1),int(c2)]\n",
    "            vote = np.sign(alphas_ovo_c1c2.T@K_test) \n",
    "            votes_ovo[:,c1,c2] = vote\n",
    "                \n",
    "    sum_votes = np.sum(votes_ovo,axis=1)\n",
    "    pred = sum_votes.argmax(axis=1)\n",
    "    diff = pred - y_test\n",
    "    mistakes = len(diff[diff!=0])\n",
    "    \n",
    "    return mistakes,pred\n",
    "\n",
    "def basic_results_ovo(d_arr, kernel_choice, runs):\n",
    "    \"\"\"\n",
    "    Produces the basic results for the one versus one perceptron approach\n",
    "    :param d_arr: an array of d values\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :param runs: the number of runs to perform to calculate the error rates\n",
    "    :return: the training and test error rates in each run\n",
    "    \"\"\"\n",
    "    training_set_errors = np.zeros((len(d_arr),runs))\n",
    "    test_set_errors = np.zeros((len(d_arr),runs))\n",
    "\n",
    "    for d in d_arr:\n",
    "        for i in range(runs):\n",
    "            startTime = datetime.now()\n",
    "            X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5,random_split=True)\n",
    "            alphas,train_errors = perceptron_train_ovo(X_train,y_train, d)\n",
    "            predictions, test_error = perceptron_test_ovo(X_test,X_train, y_test, alphas,d)\n",
    "\n",
    "            training_set_errors[d-1, i] = train_errors[-1] / X_train.shape[0]\n",
    "            test_set_errors[d-1, i] = test_error / X_test.shape[0]\n",
    "            print(\"Now doing run \", i, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "            print(\"Time taken: \", datetime.now() - startTime)\n",
    "    return training_set_errors, test_set_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now doing run  19 / 20  for d= 7 ......... Time taken:  0:00:34.482400\r"
     ]
    }
   ],
   "source": [
    "d_arr = np.arange(1,8)\n",
    "runs = 20\n",
    "training_set_errors_ovo, test_set_errors_ovo = basic_results_ovo(d_arr, 'Polynomial', runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1298 +- 0.0100</td>\n",
       "      <td>0.1383 +- 0.0128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0621 +- 0.0066</td>\n",
       "      <td>0.0819 +- 0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0370 +- 0.0036</td>\n",
       "      <td>0.0634 +- 0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0264 +- 0.0030</td>\n",
       "      <td>0.0583 +- 0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0211 +- 0.0026</td>\n",
       "      <td>0.0552 +- 0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0169 +- 0.0040</td>\n",
       "      <td>0.0521 +- 0.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0140 +- 0.0027</td>\n",
       "      <td>0.0506 +- 0.0060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training set error rate Test set error rate\n",
       "1        0.1298 +- 0.0100    0.1383 +- 0.0128\n",
       "2        0.0621 +- 0.0066    0.0819 +- 0.0092\n",
       "3        0.0370 +- 0.0036    0.0634 +- 0.0067\n",
       "4        0.0264 +- 0.0030    0.0583 +- 0.0066\n",
       "5        0.0211 +- 0.0026    0.0552 +- 0.0051\n",
       "6        0.0169 +- 0.0040    0.0521 +- 0.0079\n",
       "7        0.0140 +- 0.0027    0.0506 +- 0.0060"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_std = construct_dataframe_error_rates(training_set_errors_ovo, test_set_errors_ovo, d_arr)\n",
    "df = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation - Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation of One vs. One\n",
    "def cross_validation_ovo(matrix,d,k=5,kernel_choice='Polynomial'):\n",
    "    \"\"\"\n",
    "    Performs a k-fold cross validation on the data set, given a value d for the kernel computation.\n",
    "    :param matrix: a matrix of observations\n",
    "    :param d: the value of d used in the calculation of the kernel matrix\n",
    "    :param k: the number of splits to perform in the k-Fold cross validation\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the mean of the errors observed in every fold of the CV and its standard deviation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k)\n",
    "    error_cv_arr = np.zeros(k)\n",
    "    i=0\n",
    "    \n",
    "    for train_index, cv_index in kf.split(matrix):\n",
    "        # Spit the matrix using the indices gained by the CV method and construct X and Y arrays\n",
    "        matrix_train, matrix_cv = matrix[train_index], matrix[cv_index]\n",
    "\n",
    "        X_train = matrix_train[:,1:]\n",
    "        X_cv = matrix_cv[:,1:]\n",
    "        y_train = matrix_train[:,0] \n",
    "        y_cv = matrix_cv[:,0]\n",
    "\n",
    "        # We are only interested in the alphas and not the MSE on the training set\n",
    "        alphas, train_errors = perceptron_train_ovo(X_train,y_train, d, kernel_choice)\n",
    "        cv_errors,predictions = perceptron_test_ovo(X_cv, X_train, y_cv, alphas, d, kernel_choice)\n",
    "        \n",
    "        #print('cv_errors=',cv_errors, ' for d=',d)\n",
    "        error_cv_arr[i] = cv_errors\n",
    "        i += 1\n",
    "        \n",
    "    return error_cv_arr.mean(), (error_cv_arr.var())**(1/2)\n",
    "    \n",
    "def cv_process_ovo(data, d_arr, runs, kernel_choice):\n",
    "    \"\"\"\n",
    "    This function performs 5-fold cross validation, multiple times (specified by runs argument) across the different\n",
    "    values of d specified in d_arr using the kernel specified in kernel_choice.\n",
    "    :param d_arr: an array of d values\n",
    "    :param runs: The number of runs to repeat the CV process\n",
    "    :param kernel_choice: Depending on the kernel choice, can be {'Polynomial', 'Gaussian'}\n",
    "    :return: the array of d_stars and the test_errors\n",
    "    \"\"\"\n",
    "    d_stars = np.zeros(runs)\n",
    "    test_errors = np.zeros(runs)\n",
    "    confusions = []\n",
    "    mistakes_per_run = np.zeros(x.shape[0])\n",
    "\n",
    "    for j in range(runs):\n",
    "        confusion = np.zeros((10, 10))\n",
    "        print('run=',j)\n",
    "        # In each run we will iterate through the d array and use all possible values of d\n",
    "\n",
    "        # Allocate 80/20 percent for training and test set\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "        data_train = np.zeros((X_train.shape[0],X_train.shape[1]+1))\n",
    "        data_train[:,0] = y_train\n",
    "        data_train[:,1:] = X_train\n",
    "        \n",
    "        CV_means = np.zeros(len(d_arr))\n",
    "        #Only apply cross validation on training data\n",
    "        for i in range(len(d_arr)):\n",
    "            print(\"d=\", d_arr[i])\n",
    "            #print(\"Now doing run \", j+1, \"/\", runs, \" for d=\", d_arr[i], \".........\", end='\\r')\n",
    "            errors_CV_mean, _ = cross_validation_ovo(data_train, d_arr[i], k=5, kernel_choice='Polynomial')\n",
    "            CV_means[i] = errors_CV_mean  \n",
    "\n",
    "        # Train in whole 80% now with d_star\n",
    "        d_stars[j] = d_arr[CV_means.argmin()]\n",
    "        alphas, errors = perceptron_train_ovo(X_train, y_train, d_stars[j], kernel_choice = kernel_choice)\n",
    "\n",
    "        mistakes,_ = perceptron_test_ovo(X_test, X_train, y_test, alphas, d_stars[j], kernel_choice = kernel_choice)\n",
    "        test_errors[j] = mistakes / len(y_test)\n",
    "        \n",
    "    return d_stars, test_errors, confusions, mistakes_per_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "d_arr = np.arange(1,8)\n",
    "d_stars, test_errors, confusions, mistakes_per_run = cv_process_ovo(data, d_arr, runs, 'Polynomial')\n",
    "\n",
    "# Uncomment these lines to load pre-saved data if not wanting to spend too much time on the above process\n",
    "# d_stars = pd.read_pickle('d_stars_cv_ovo').values\n",
    "# test_errors = pd.read_pickle('test_errors_cv_ovo').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean d*:  6.15  with std:  0.9096702699330126\n",
      "Mean test error:  0.04935483870967742  with std:  0.0044516648482762254\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean d*: \", d_stars.mean(), \" with std: \", np.std(d_stars))\n",
    "print(\"Mean test error: \", test_errors.mean(), \" with std: \", np.std(test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 - Choose two more algorithms to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:30:25.036917 c= 1000 ..........\n"
     ]
    }
   ],
   "source": [
    "def basic_results_logistic_regression(cost_range, runs):\n",
    "    \"\"\"\n",
    "    For every value in cost_range, it performs \"runs\" iterations where it trains a perceptron,\n",
    "    based on randomly selected 80% of the data and tests on the remaining 20%.\n",
    "    :param cost_range: an array of C values\n",
    "    :param runs: the number of runs to perform\n",
    "    :return: two arrays: one containing the errors recorded in the training set in every run and one for the test set.\n",
    "    \"\"\"\n",
    "    training_set_errors = np.zeros((len(cost_range), runs))\n",
    "    test_set_errors = np.zeros((len(cost_range), runs))\n",
    "    \n",
    "    for c in range(len(cost_range)):\n",
    "        cost = cost_range[c]\n",
    "        for i in range(runs):\n",
    "            print(\"Now doing run \", i + 1, \"/\", runs, \" for c=\", cost, \".........\", end='\\r')\n",
    "            X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1 / 5)\n",
    "            lr = LogisticRegression(C=cost,\n",
    "                                    random_state=0,\n",
    "                                    solver='newton-cg',\n",
    "                                    multi_class='multinomial')\n",
    "            lr.fit(X_train, y_train)\n",
    "\n",
    "            test_labels = lr.predict(X_test)\n",
    "            test_errors = sum(y_test != test_labels) / y_test.shape[0]\n",
    "\n",
    "            train_labels = lr.predict(X_train)\n",
    "            train_errors = sum(y_train != train_labels) / y_train.shape[0]\n",
    "\n",
    "            training_set_errors[c, i] = train_errors\n",
    "            test_set_errors[c, i] = test_errors\n",
    "                \n",
    "    return training_set_errors, test_set_errors\n",
    "\n",
    "cost_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "runs = 20\n",
    "startTime = datetime.now()\n",
    "training_set_errors_lr, test_set_errors_lr = basic_results_logistic_regression(cost_range, runs)\n",
    "time_lr_basic = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_lr_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.0817 +- 0.0014</td>\n",
       "      <td>0.0847 +- 0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.0510 +- 0.0009</td>\n",
       "      <td>0.0635 +- 0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0285 +- 0.0009</td>\n",
       "      <td>0.0551 +- 0.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.0088 +- 0.0011</td>\n",
       "      <td>0.0644 +- 0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.0007 +- 0.0002</td>\n",
       "      <td>0.0753 +- 0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.0001 +- 0.0001</td>\n",
       "      <td>0.0834 +- 0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.000</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0864 +- 0.0052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Training set error rate Test set error rate\n",
       "0.001           0.0817 +- 0.0014    0.0847 +- 0.0054\n",
       "0.010           0.0510 +- 0.0009    0.0635 +- 0.0054\n",
       "0.100           0.0285 +- 0.0009    0.0551 +- 0.0033\n",
       "1.000           0.0088 +- 0.0011    0.0644 +- 0.0063\n",
       "10.000          0.0007 +- 0.0002    0.0753 +- 0.0057\n",
       "100.000         0.0001 +- 0.0001    0.0834 +- 0.0047\n",
       "1000.000        0.0000 +- 0.0000    0.0864 +- 0.0052"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_std = construct_dataframe_error_rates(training_set_errors_lr, test_set_errors_lr, cost_range)\n",
    "df = pd.DataFrame(data=means_std, index=cost_range, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_cv(cost_range):\n",
    "    \"\"\"\n",
    "    Performs a 5-Fold cross-validation in 80% of the data, iterating through different values of C found in cost_range.\n",
    "    Upon finding the optimal value of C, c_star, it trains on the whole 80% of the data and calculated the error rate\n",
    "    on the test set.\n",
    "    :param cost_range: an array of values for parameter C in Logistic Regression\n",
    "    :return: the optimal value for C and the test error observed in the test set\n",
    "    \"\"\"\n",
    "    K = 5  # number of folds for cross validation\n",
    "    kf = KFold(n_splits=K, shuffle=True)\n",
    "    cv_error = np.zeros(len(cost_range))  # error matrix\n",
    "\n",
    "    # Allocate 80% for the CV process\n",
    "    X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1 / 5)\n",
    "\n",
    "    for j in range(len(cost_range)):\n",
    "        c = cost_range[j]\n",
    "        cumulative_error = 0\n",
    "        cv_i = 0\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            print(\"Starting CV round\", cv_i + 1, \" for c=\", c, \"...\", end='\\r')\n",
    "            # Get the training/test set as given by the cross validation\n",
    "            X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
    "            y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            lr = LogisticRegression(C=c,\n",
    "                                    random_state=0,\n",
    "                                    solver='newton-cg',\n",
    "                                    multi_class='multinomial')\n",
    "            lr.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "            predicted_labels = lr.predict(X_test_cv)\n",
    "\n",
    "            # Calculate the error on the test set\n",
    "            error = sum(y_test_cv != predicted_labels)\n",
    "            cumulative_error += error\n",
    "            cv_i += 1\n",
    "\n",
    "        # Divide by K to get the average and store it in the i-th (gamma) row, j-th (cost) column\n",
    "        cv_error[j] = cumulative_error / K\n",
    "\n",
    "    cost_star = cost_range[cv_error.argmin()]\n",
    "\n",
    "    # Train on whole of 80%\n",
    "    lr = LogisticRegression(C=cost_star,\n",
    "                            random_state=0,\n",
    "                            solver='newton-cg',\n",
    "                            multi_class='multinomial')\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    predicted_labels = lr.predict(X_test)\n",
    "    test_error = sum(y_test != predicted_labels) / y_test.shape[0]\n",
    "\n",
    "    return cost_star, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  1 / 20\n",
      "Run:  2 / 20round 5  for c= 1000 ....\n",
      "Run:  3 / 20round 5  for c= 1000 ....\n",
      "Run:  4 / 20round 5  for c= 1000 ....\n",
      "Run:  5 / 20round 5  for c= 1000 ....\n",
      "Run:  6 / 20round 5  for c= 1000 ....\n",
      "Run:  7 / 20round 5  for c= 1000 ....\n",
      "Run:  8 / 20round 5  for c= 1000 ....\n",
      "Run:  9 / 20round 5  for c= 1000 ....\n",
      "Run:  10 / 20ound 5  for c= 1000 ....\n",
      "Run:  11 / 20ound 5  for c= 1000 ....\n",
      "Run:  12 / 20ound 5  for c= 1000 ....\n",
      "Run:  13 / 20ound 5  for c= 1000 ....\n",
      "Run:  14 / 20ound 5  for c= 1000 ....\n",
      "Run:  15 / 20ound 5  for c= 1000 ....\n",
      "Run:  16 / 20ound 5  for c= 1000 ....\n",
      "Run:  17 / 20ound 5  for c= 1000 ....\n",
      "Run:  18 / 20ound 5  for c= 1000 ....\n",
      "Run:  19 / 20ound 5  for c= 1000 ....\n",
      "Run:  20 / 20ound 5  for c= 1000 ....\n",
      "Time taken:  1:02:16.431179 1000 ....\n"
     ]
    }
   ],
   "source": [
    "runs = 20\n",
    "test_errors_lr = np.zeros(runs)\n",
    "cost_stars_lr = np.zeros(runs)\n",
    "\n",
    "startTime = datetime.now()\n",
    "for i in range(runs):\n",
    "    print(\"Run: \", i+1, \"/\", runs)\n",
    "    cost_star, test_error = logistic_regression_cv(cost_range)\n",
    "    test_errors_lr[i] = test_error\n",
    "    cost_stars_lr[i] = cost_star\n",
    "    \n",
    "time_lr_cv = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test error: 0.05489247311827957 +- 0.004377347907383844\n",
      "Cost: 0.10000000000000002 +- 1.3877787807814457e-17\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean test error: \", test_errors_lr.mean(), \" +- \", np.std(test_errors_lr))\n",
    "print(\"Cost: \", cost_stars_lr.mean(), \" +- \", np.std(cost_stars_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:32:16.598337  rbf , c= 1000 .........\n",
      "Time taken:  0:28:51.367349  poly , c= 1000 .........\n"
     ]
    }
   ],
   "source": [
    "def basic_results_svm(cost_range, kernels, runs):\n",
    "    \"\"\"\n",
    "    Performs 'runs' iterations over the data set and each iteration:\n",
    "    - for every kernel choice found in kernels array\n",
    "        - for every value of C found in cost_range array\n",
    "            Trains an SVM predictor with this combination on a random 80% portion of the data.\n",
    "            Evaluates the prediction error as observed in the remaining 20% of the data.\n",
    "    :param cost_range: an array of C values\n",
    "    :param kernels: an array of kernel choices (must be a supported value in scikit-learn's SVC implementation)\n",
    "    :param runs: the number of iterations to perform\n",
    "    :return: two arrays: the error rates as observed in the training set and test set respectively.\n",
    "    \"\"\"\n",
    "    training_set_errors = np.zeros((len(kernels), len(cost_range), runs))\n",
    "    test_set_errors = np.zeros((len(kernels), len(cost_range), runs))\n",
    "    \n",
    "    for k in range(len(kernels)):\n",
    "        kernel = kernels[k]\n",
    "        for c in range(len(cost_range)):\n",
    "            cost = cost_range[c]\n",
    "            for i in range(runs):\n",
    "                print(\"Now doing run \", i+1, \"/\", runs, \" for \", kernel,\", c=\",cost,\".........\", end='\\r')\n",
    "                X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "                svm_classifier = svm.SVC(C=cost, kernel=kernel, gamma='scale')\n",
    "                svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "                test_labels = svm_classifier.predict(X_test)\n",
    "                test_errors = sum(y_test != test_labels) / len(y_test)\n",
    "                \n",
    "                train_labels = svm_classifier.predict(X_train)\n",
    "                train_errors = sum(y_train != train_labels) / len(y_train)\n",
    "                \n",
    "                training_set_errors[k, c, i] = train_errors\n",
    "                test_set_errors[k, c, i] = test_errors\n",
    "                \n",
    "    return training_set_errors, test_set_errors\n",
    "\n",
    "cost_range = [ 0.01, 0.1, 10, 100, 1000]\n",
    "kernels = [\"rbf\", \"poly\"]\n",
    "tuned_parameters = [{'kernel': [kernels[0]], 'C': cost_range},\n",
    "                    {'kernel': [kernels[1]], 'C': cost_range}\n",
    "                   ]\n",
    "runs = 20\n",
    "\n",
    "startTime = datetime.now()\n",
    "svm_training_set_errors_rbf, svm_test_set_errors_rbf = basic_results_svm(cost_range, kernels[:1], runs)\n",
    "time_svm_rbf_basic = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_svm_rbf_basic)\n",
    "\n",
    "startTime = datetime.now()\n",
    "svm_training_set_errors_poly, svm_test_set_errors_poly = basic_results_svm(cost_range, kernels[1:], runs)\n",
    "time_svm_poly_basic = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_svm_poly_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel:  rbf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.2166 +- 0.0040</td>\n",
       "      <td>0.2241 +- 0.0123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.0477 +- 0.0012</td>\n",
       "      <td>0.0563 +- 0.0059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00</th>\n",
       "      <td>0.0004 +- 0.0001</td>\n",
       "      <td>0.0224 +- 0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.00</th>\n",
       "      <td>0.0001 +- 0.0001</td>\n",
       "      <td>0.0228 +- 0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.00</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0233 +- 0.0039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Training set error rate Test set error rate\n",
       "0.01           0.2166 +- 0.0040    0.2241 +- 0.0123\n",
       "0.10           0.0477 +- 0.0012    0.0563 +- 0.0059\n",
       "10.00          0.0004 +- 0.0001    0.0224 +- 0.0018\n",
       "100.00         0.0001 +- 0.0001    0.0228 +- 0.0036\n",
       "1000.00        0.0000 +- 0.0000    0.0233 +- 0.0039"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_means_std = construct_dataframe_error_rates(svm_training_set_errors[0,:,:], svm_test_set_errors[0,:,:], cost_range)\n",
    "print(\"Kernel: \", kernels[0])\n",
    "df = pd.DataFrame(data=svm_means_std, index=cost_range, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel:  poly\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.1717 +- 0.0040</td>\n",
       "      <td>0.1737 +- 0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.0380 +- 0.0009</td>\n",
       "      <td>0.0485 +- 0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00</th>\n",
       "      <td>0.0002 +- 0.0001</td>\n",
       "      <td>0.0211 +- 0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.00</th>\n",
       "      <td>0.0001 +- 0.0000</td>\n",
       "      <td>0.0207 +- 0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.00</th>\n",
       "      <td>0.0000 +- 0.0000</td>\n",
       "      <td>0.0210 +- 0.0029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Training set error rate Test set error rate\n",
       "0.01           0.1717 +- 0.0040    0.1737 +- 0.0066\n",
       "0.10           0.0380 +- 0.0009    0.0485 +- 0.0035\n",
       "10.00          0.0002 +- 0.0001    0.0211 +- 0.0027\n",
       "100.00         0.0001 +- 0.0000    0.0207 +- 0.0034\n",
       "1000.00        0.0000 +- 0.0000    0.0210 +- 0.0029"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_means_std = construct_dataframe_error_rates(svm_training_set_errors[1,:,:], svm_test_set_errors[1,:,:], cost_range)\n",
    "print(\"Kernel: \", kernels[1])\n",
    "df = pd.DataFrame(data=svm_means_std, index=cost_range, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cv(tuned_parameters, cost_range):\n",
    "    \"\"\"\n",
    "    Performs a 5-fold cross validation over a random 80% split of the data set so as to find the best combination\n",
    "    of tuned_parameters (i.e. combination of kernel and cost value C). Upon finding the best combination for the SVM,\n",
    "    it trains on the whole 80% of the training set and evaluates the error rate on the test set.\n",
    "    :param tuned_parameters: an array of parameter combinations to evaluate\n",
    "    :param cost_range: an array of values for C\n",
    "    :return: the optimal combination of kernel and cost as well as the test error observed.\n",
    "    \"\"\"\n",
    "    K = 5  # number of folds for cross validation\n",
    "    kf = KFold(n_splits=K, shuffle=True)\n",
    "    cv_error = np.zeros((len(tuned_parameters), len(cost_range)))  # error matrix\n",
    "\n",
    "    # Allocate 80% for the CV process\n",
    "    X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "    for i in range(len(tuned_parameters)):\n",
    "        parameters = tuned_parameters[i]\n",
    "        kernel = parameters['kernel'][0]\n",
    "        c_range = parameters['C']\n",
    "\n",
    "        for j in range(len(c_range)):    \n",
    "            c = c_range[j]\n",
    "            cummulative_error = 0\n",
    "            cv_i = 0\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                print(\"Starting CV round\", cv_i+1, \" for \", kernel, \" c=\", c, \"...\", end='\\r')\n",
    "                # Get the training/test set as given by the cross validation\n",
    "                X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
    "                y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "\n",
    "                # Train an SVM classifier on the training test\n",
    "                svm_classifier = svm.SVC(C=c, kernel=kernel, gamma='scale')\n",
    "                svm_classifier.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "                # Predict on the test set\n",
    "                predicted_labels = svm_classifier.predict(X_test_cv)\n",
    "\n",
    "                # Calculate the error on the test set\n",
    "                error = sum(y_test_cv != predicted_labels)\n",
    "                cummulative_error += error\n",
    "                cv_i+=1\n",
    "\n",
    "            # Divide by K to get the average and store it in the i-th (gamma) row, j-th (cost) column\n",
    "            cv_error[i][j] = cummulative_error / K\n",
    "\n",
    "    indices = np.argwhere(cv_error == cv_error.min())[0]\n",
    "\n",
    "    # This is the index of the optimal gammas, costs\n",
    "    kernel_star = tuned_parameters[indices[0]]['kernel'][0]\n",
    "    cost_star = tuned_parameters[indices[0]]['C'][indices[1]]\n",
    "\n",
    "    # Train on whole of 80%\n",
    "    svm_classifier = svm.SVC(C=cost_star, kernel=kernel_star, gamma='scale')\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    predicted_labels = svm_classifier.predict(X_test)\n",
    "    test_error = sum(y_test != predicted_labels) / len(y_test)\n",
    "\n",
    "    return kernel_star, cost_star, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run:  1  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  2  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  3  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  4  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  5  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  6  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  7  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  8  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  9  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  10  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  11  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  12  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  13  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  14  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  15  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  16  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  17  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  18  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  19  / 20\n",
      "Starting CV round 5  for  poly  c= 1000 ...\n",
      "Run:  20  / 20\n",
      "Time taken:  2:04:09.670684oly  c= 1000 ...\n"
     ]
    }
   ],
   "source": [
    "runs = 20\n",
    "svm_test_errors = np.zeros(runs)\n",
    "svm_kernel_stars = np.zeros(runs)\n",
    "svm_cost_stars = np.zeros(runs)\n",
    "\n",
    "startTime = datetime.now()\n",
    "for i in range(runs):\n",
    "    print(\"\\nRun: \", i+1, \" /\", runs)\n",
    "    kernel_star, cost_star, test_error = svm_cv(tuned_parameters, cost_range)\n",
    "    \n",
    "    svm_test_errors[i] = test_error\n",
    "    svm_kernel_stars[i] = kernels.index(kernel_star)\n",
    "    svm_cost_stars[i] = cost_star\n",
    "\n",
    "time_svm_cv = datetime.now() - startTime\n",
    "print(\"Time taken: \", time_svm_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test error:  0.022553763440860214  +-  0.0030953691182203365\n",
      "Kernel choice (0=rbf, 1=linear): 0.8  +-  0.4\n",
      "C value (log10 scale):  1.85  +-  0.852936105461599\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean test error: \", svm_test_errors.mean(), \" +- \", np.std(svm_test_errors))\n",
    "print(\"Kernel choice (0=rbf, 1=linear):\", svm_kernel_stars.mean(), \" +- \", np.std(svm_kernel_stars))\n",
    "print(\"C value (log10 scale): \", np.log10(svm_cost_stars).mean(), \" +- \", np.std(np.log10(svm_cost_stars)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
