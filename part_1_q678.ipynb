{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approaches to Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution of data\n",
    "# Batch training\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = \"http://www0.cs.ucl.ac.uk/staff/M.Herbster/SL/misc/zipcombo.dat\"\n",
    "filename = 'zipcombo.dat'\n",
    "urllib.request.urlretrieve(link, filename)\n",
    "data = np.loadtxt(filename)     # read numpy array from fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[:,0]\n",
    "x = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 - Alternative generalization method, One vs. One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allocate_training_test_sets(data,r =1/5, random_split=False):\n",
    "    if random_split:\n",
    "        np.random.shuffle(data)\n",
    "    X= data[:,1:]\n",
    "    y= data[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=r)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def add_bias(x):\n",
    "    x_with_bias = np.ones((x.shape[0],x.shape[1]+1))\n",
    "    x_with_bias[:,:-1] = x\n",
    "    return x_with_bias\n",
    "\n",
    "\n",
    "def calculate_kernel_double(x1, x2, d, kernel_choice):\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x1,x2,d)\n",
    "#         print(\"Constructed a Polynomial kernel for testing\")\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_double(x1, x2)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "#         print(\"Constructed a Gaussian kernel for testing\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "\n",
    "\n",
    "def calculate_kernel_single(x, d, kernel_choice):\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x,x,d)\n",
    "#         print(\"Constructed a Polynomial kernel for training\")\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_single(x)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "#         print(\"Constructed a Gaussian kernel for training\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "\n",
    "#Discuss the use of the this kernel. i.e. talk about non-linear seperability. \n",
    "def Polynomial_Kernel(x1,x2,d):\n",
    "    K = (x1 @ x2.T)**d\n",
    "    return K\n",
    "\n",
    "def pairwise_distance_single(X): # distances of X training data, single X matrix\n",
    "    m =X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    G = np.matmul(X,X.T)\n",
    "    DG = np.diag(G).reshape(G.shape[0],1)\n",
    "    distances_sq = np.matmul(DG,np.ones((G.shape[0],1)).T)+ np.matmul(np.ones((G.shape[1],1)),DG.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def pairwise_distance_double(X1,X2): # distances of X training data, double matrices, X1 and X2\n",
    "    X1_pow = (X1**2).sum(axis=1).reshape(X1.shape[0],1) #sum the rows, size m1 array\n",
    "    X2_pow = (X2**2).sum(axis=1).reshape(X2.shape[0],1) #sum the rows, size m2 array\n",
    "    G = np.matmul(X1,X2.T)\n",
    "    m1,m2 =G.shape[0],G.shape[1] \n",
    "    distances_sq = np.matmul(X1_pow,np.ones((m2,1)).T)+ np.matmul(np.ones((m1,1)),X2_pow.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def Gaussian_Kernel(distances_sq,c=1):\n",
    "    K = np.exp(-c*distances_sq)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs. One Pairwise Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One vs. one approach: train k*(k-1)/2 binary classifiers to identify k classes\n",
    "#For example, one classifier could be trained to distinguish between digit 0 and digit 1.\n",
    "#Consider symmetry when computing prediction\n",
    "\n",
    "#Write a function that trains a binary classifier given two classes, given kernel:\n",
    "def classifier_ovo(class1,class2,K,alpha_ovo,iter_num):\n",
    "    vote = np.sign(((alpha_ovo[:].T @K[iter_num,:]).T))\n",
    "    return vote #returns a vote, within (-1,1)\n",
    "\n",
    "def perceptron_train_ovo(x,y,d=2,kernel_choice='Polynomial',max_epoch=10, tol=0.01):\n",
    "    m = x.shape[0] #number of examples\n",
    "    n = x.shape[1] #number of features\n",
    "    classes_num = 10 #number of classes \n",
    "    error_per_epoch = np.zeros(max_epoch)\n",
    "    errors = np.zeros(m)\n",
    "    \n",
    "    K_train = calculate_kernel_single(x, d, kernel_choice)\n",
    "    \n",
    "    num_errors = 0 \n",
    "    alpha = np.zeros((m,classes_num,classes_num)) \n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        errors = np.zeros(m)\n",
    "        num_errors = 0 #This should be bounded..? Maybe calculate the bound in the explanation\n",
    "    \n",
    "        #iterate through training set\n",
    "        for t in range(m):\n",
    "            if t<1:\n",
    "                alpha_prev = alpha[0,:,:] #when t=0, the previous alpha is set to be 0\n",
    "            else:\n",
    "                alpha_prev = alpha[t-1,:,:] #\n",
    "\n",
    "            x_t = x[t,:]\n",
    "            y_t = y[t]\n",
    "\n",
    "            votes_board = np.zeros((classes_num, classes_num)) #zero on the horizontal. \n",
    "            classes_list = np.array(range(classes_num))\n",
    "\n",
    "\n",
    "            for i in range(classes_num):\n",
    "                c1 = classes_list[i]\n",
    "                classes_rest = classes_list[classes_list>c1]\n",
    "                for j in range(len(classes_rest)):\n",
    "                    c2 = classes_rest[j]\n",
    "                    alpha_ovo = alpha[:,c1,c2]\n",
    "                    vote = classifier_ovo(c1,c2,K_train,alpha_ovo,iter_num=t)\n",
    "                    votes_board[c1,c2] = vote\n",
    "\n",
    "            #Count the votes in the board\n",
    "            votes_count = votes_board.sum(axis=0)\n",
    "            pred_t = votes_count.argmax()\n",
    "\n",
    "            if pred_t!=y_t:\n",
    "                num_errors +=1\n",
    "\n",
    "                #increase alpha for all the positive classifier of the correct label.\n",
    "                #decrease alpha for the negative classifier of the false label. \n",
    "                alpha_t = alpha_prev #initialize it to its previous form\n",
    "                alpha_t[:,int(y_t)] =+1 # column belonging to correct label class +=1\n",
    "                alpha_t[:,int(pred_t)] =-1 # column belonging to false predicted class -=1\n",
    "\n",
    "                #store alpha_t into the matrix for future reference\n",
    "                alpha[t,:,:] = alpha_t\n",
    "            \n",
    "            errors[t] = num_errors \n",
    "        \n",
    "        error_per_epoch[epoch] = errors[-1]\n",
    "        #print(epoch, 'error=',errors[-1])\n",
    "        \n",
    "        if epoch>1:\n",
    "            diff_rates = (error_per_epoch[epoch-1] - error_per_epoch[epoch])/m\n",
    "            \n",
    "            #Stop if the error rate has increased, \n",
    "            #or the difference in error rate between the previous one and the current one < tolerance. \n",
    "            if diff_rates<tol or diff_rates<0:\n",
    "                print('difference in error rate', diff_rates)\n",
    "                print('break point at epoch=', epoch )\n",
    "                break\n",
    "        \n",
    "    return alpha, error_per_epoch[:epoch+1]\n",
    "\n",
    "\n",
    "\n",
    "def perceptron_test_ovo(x_test,x_train,y_test,alphas, d,kernel_choice='Polynomial'):\n",
    "    m_test = x_test.shape[0]\n",
    "    m_train = x_train.shape[0]\n",
    "    \n",
    "    K_test = calculate_kernel_double(x_train, x_test, d, kernel_choice)\n",
    "    \n",
    "    classes_num = 10\n",
    "    classes_list = np.array(range(classes_num))\n",
    "    votes_ovo =np.zeros((m_test,10,10))\n",
    "    \n",
    "    for i in range(classes_num):\n",
    "        c1 = classes_list[i]\n",
    "        classes_rest = classes_list[classes_list>c1]\n",
    "        for j in range(len(classes_rest)):\n",
    "            c2 = classes_rest[j]\n",
    "            alphas_ovo_c1c2 = alphas[:,int(c1),int(c2)]\n",
    "            vote = np.sign(alphas_ovo_c1c2.T@K_test) \n",
    "            votes_ovo[:,c1,c2] = vote\n",
    "                \n",
    "    sum_votes = np.sum(votes_ovo,axis=1)\n",
    "    pred = sum_votes.argmax(axis=1)\n",
    "    diff = pred - y_test\n",
    "    mistakes = len(diff[diff!=0])\n",
    "    \n",
    "    return mistakes,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataframe_error_rates(training_set_errors, test_set_errors):\n",
    "    means_std = []\n",
    "    for d in d_arr:\n",
    "        data_t = []\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(training_set_errors[d-1].mean(), \\\n",
    "                                                np.std(training_set_errors[d-1])))\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(test_set_errors[d-1].mean(), \\\n",
    "                                                np.std(test_set_errors[d-1])))\n",
    "        means_std.append(data_t)\n",
    "    return means_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_results(d_arr, kernel_choice, runs):\n",
    "    training_set_errors = np.zeros((len(d_arr),runs))\n",
    "    test_set_errors = np.zeros((len(d_arr),runs))\n",
    "    for d in d_arr:\n",
    "        for i in range(runs):\n",
    "            print(\"Now doing run \", i+1, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "            X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "            \n",
    "            alphas,train_errors = perceptron_train_ovo(X_train,y_train, d)\n",
    "            predictions, test_error = perceptron_test_ovo(X_test,X_train, y_test, alphas,d)\n",
    "\n",
    "            #train_errors,_,_ = perceptron_test(X_train, X_train, y_train, alphas,d, kernel_choice=kernel_choice)\n",
    "            #test_errors,_,_ = perceptron_test(X_test,X_train, y_test,alphas,d, kernel_choice=kernel_choice)\n",
    "\n",
    "            training_set_errors[d-1, i] = train_errors / len(y_train)\n",
    "            test_set_errors[d-1, i] = test_errors / len(y_test)\n",
    "    return training_set_errors, test_set_errors\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on all training data and test data, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in error rate 0.009545576767948373\n",
      "break point at epoch= 3\n",
      "0:01:13.539959\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "X_train, X_test, y_train, y_test = allocate_training_test_sets(data,r =1/5)\n",
    "alphas_ovo, train_errors = perceptron_train_ovo(X_train,y_train, d=2,max_epoch=10)\n",
    "test_errors,predictions = perceptron_test_ovo(X_test, X_train, y_test, alphas_ovo, 2)\n",
    "print(datetime.now() - startTime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1411.,  700.,  578.,  507.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a27579828>]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Results  - Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:02:06.850727d= 1 .........\n",
      "Time taken:  0:01:34.001226d= 1 .........\n",
      "Time taken:  0:01:30.258883d= 1 .........\n",
      "Time taken:  0:01:31.067757d= 1 .........\n",
      "Time taken:  0:01:45.497868d= 1 .........\n",
      "Time taken:  0:01:29.221478d= 1 .........\n",
      "Time taken:  0:01:29.647567d= 1 .........\n",
      "Time taken:  0:37:47.489077d= 1 .........\n",
      "Time taken:  0:02:08.405380d= 1 .........\n",
      "Time taken:  0:02:11.632202d= 1 .........\n",
      "Time taken:  0:02:20.107663 d= 1 .........\n",
      "Time taken:  0:01:36.432795 d= 1 .........\n",
      "Time taken:  0:01:31.106984 d= 1 .........\n",
      "Time taken:  0:01:41.431903 d= 1 .........\n",
      "Time taken:  0:02:05.996656 d= 1 .........\n",
      "Time taken:  0:01:37.294993 d= 1 .........\n",
      "Time taken:  0:01:47.028888 d= 1 .........\n",
      "Time taken:  0:02:17.003802 d= 1 .........\n",
      "Time taken:  0:02:00.293113 d= 1 .........\n",
      "Time taken:  0:01:43.101110 d= 1 .........\n",
      "Time taken:  0:01:50.043767d= 2 .........\n",
      "Time taken:  0:02:00.766049d= 2 .........\n",
      "Time taken:  0:01:31.958548d= 2 .........\n",
      "Time taken:  0:01:48.359575d= 2 .........\n",
      "Time taken:  0:01:53.189985d= 2 .........\n",
      "Time taken:  0:02:17.467815d= 2 .........\n",
      "Time taken:  0:01:58.354250d= 2 .........\n",
      "Time taken:  0:02:16.403180d= 2 .........\n",
      "Time taken:  0:02:26.140234d= 2 .........\n",
      "Time taken:  0:01:55.932878d= 2 .........\n",
      "Time taken:  0:01:49.616222 d= 2 .........\n",
      "Time taken:  0:01:50.513306 d= 2 .........\n",
      "Time taken:  0:01:53.009829 d= 2 .........\n",
      "Time taken:  0:01:49.357510 d= 2 .........\n",
      "Time taken:  0:02:16.144103 d= 2 .........\n",
      "Time taken:  0:02:39.298476 d= 2 .........\n",
      "Time taken:  0:02:10.666657 d= 2 .........\n",
      "Time taken:  0:02:10.121578 d= 2 .........\n",
      "Time taken:  0:02:25.695316 d= 2 .........\n",
      "Time taken:  0:02:30.826478 d= 2 .........\n",
      "Time taken:  0:01:46.805240d= 3 .........\n",
      "Time taken:  0:01:51.656468d= 3 .........\n",
      "Time taken:  0:01:43.690318d= 3 .........\n",
      "Time taken:  0:01:41.586181d= 3 .........\n",
      "Time taken:  0:01:28.500553d= 3 .........\n",
      "Time taken:  0:01:58.240910d= 3 .........\n",
      "Time taken:  0:01:52.754168d= 3 .........\n",
      "Time taken:  0:01:27.862636d= 3 .........\n",
      "Time taken:  0:01:44.105281d= 3 .........\n",
      "Time taken:  0:01:30.130777d= 3 .........\n",
      "Time taken:  0:02:00.501362 d= 3 .........\n",
      "Time taken:  0:02:53.273166 d= 3 .........\n",
      "Time taken:  0:02:27.548324 d= 3 .........\n",
      "Time taken:  0:01:55.743130 d= 3 .........\n",
      "Time taken:  0:01:47.774720 d= 3 .........\n",
      "Time taken:  0:01:36.442807 d= 3 .........\n",
      "Time taken:  0:01:44.653598 d= 3 .........\n",
      "Time taken:  0:01:48.296969 d= 3 .........\n",
      "Time taken:  0:02:05.146101 d= 3 .........\n",
      "Time taken:  0:01:28.551091 d= 3 .........\n",
      "Time taken:  0:01:37.524749d= 4 .........\n",
      "Time taken:  0:01:27.574453d= 4 .........\n",
      "Time taken:  0:01:28.292787d= 4 .........\n",
      "Time taken:  0:01:27.942754d= 4 .........\n",
      "Time taken:  0:01:28.632976d= 4 .........\n",
      "Time taken:  2:01:43.840124d= 4 .........\n",
      "Time taken:  0:41:58.816528d= 4 .........\n",
      "Time taken:  6:01:37.842771d= 4 .........\n",
      "Time taken:  2:07:53.915923d= 4 .........\n",
      "Time taken:  0:01:34.436600d= 4 .........\n",
      "Time taken:  0:03:27.632577 d= 4 .........\n",
      "Time taken:  0:02:47.286837 d= 4 .........\n",
      "Time taken:  0:01:51.324462 d= 4 .........\n",
      "Time taken:  0:02:57.404245 d= 4 .........\n",
      "Time taken:  0:02:13.399258 d= 4 .........\n",
      "Time taken:  0:02:14.520158 d= 4 .........\n",
      "Time taken:  0:02:13.481843 d= 4 .........\n",
      "Time taken:  0:02:07.450351 d= 4 .........\n",
      "Time taken:  0:01:59.674105 d= 4 .........\n",
      "Time taken:  0:02:24.406137 d= 4 .........\n",
      "Time taken:  0:02:17.549034d= 5 .........\n",
      "Time taken:  0:02:43.453675d= 5 .........\n",
      "Time taken:  0:03:06.719847d= 5 .........\n",
      "Time taken:  0:01:55.205223d= 5 .........\n",
      "Time taken:  0:02:19.244408d= 5 .........\n",
      "Time taken:  0:02:05.441542d= 5 .........\n",
      "Time taken:  0:01:56.106333d= 5 .........\n",
      "Time taken:  0:01:44.056123d= 5 .........\n",
      "Time taken:  0:01:40.267306d= 5 .........\n",
      "Time taken:  0:01:46.268214d= 5 .........\n",
      "Time taken:  0:01:46.394773 d= 5 .........\n",
      "Time taken:  0:01:46.734112 d= 5 .........\n",
      "Time taken:  0:02:19.535669 d= 5 .........\n",
      "Time taken:  0:02:12.995980 d= 5 .........\n",
      "Time taken:  0:01:40.640696 d= 5 .........\n",
      "Time taken:  0:01:49.781665 d= 5 .........\n",
      "Time taken:  0:02:16.134023 d= 5 .........\n",
      "Time taken:  0:01:42.875539 d= 5 .........\n",
      "Time taken:  0:03:02.054181 d= 5 .........\n",
      "Time taken:  0:02:32.294809 d= 5 .........\n",
      "Time taken:  0:01:41.519595d= 6 .........\n",
      "Time taken:  0:03:46.208583d= 6 .........\n",
      "Time taken:  0:02:18.865883d= 6 .........\n",
      "Time taken:  0:02:18.113948d= 6 .........\n",
      "Time taken:  0:02:01.599529d= 6 .........\n",
      "Time taken:  0:02:06.558015d= 6 .........\n",
      "Time taken:  0:01:58.664370d= 6 .........\n",
      "Time taken:  0:02:19.882227d= 6 .........\n",
      "Time taken:  0:02:45.799483d= 6 .........\n",
      "Time taken:  0:02:26.759261d= 6 .........\n",
      "Time taken:  0:02:08.920570 d= 6 .........\n",
      "Time taken:  0:02:22.742971 d= 6 .........\n",
      "Time taken:  0:02:24.041303 d= 6 .........\n",
      "Time taken:  0:01:51.073926 d= 6 .........\n",
      "Time taken:  0:02:34.833052 d= 6 .........\n",
      "Time taken:  0:02:22.257708 d= 6 .........\n",
      "Time taken:  0:03:04.623419 d= 6 .........\n",
      "Time taken:  0:03:46.692598 d= 6 .........\n",
      "Time taken:  0:02:36.150449 d= 6 .........\n",
      "Time taken:  0:02:55.663773 d= 6 .........\n",
      "Time taken:  0:02:37.955965d= 7 .........\n",
      "Time taken:  0:03:12.572248d= 7 .........\n",
      "Time taken:  0:03:15.551220d= 7 .........\n",
      "Time taken:  0:02:51.438215d= 7 .........\n",
      "Time taken:  0:02:09.583653d= 7 .........\n",
      "Time taken:  0:02:14.031521d= 7 .........\n",
      "Time taken:  0:01:27.567330d= 7 .........\n",
      "Time taken:  0:01:50.295126d= 7 .........\n",
      "Time taken:  0:01:30.063281d= 7 .........\n",
      "Time taken:  0:01:31.076848d= 7 .........\n",
      "Time taken:  0:01:30.178759 d= 7 .........\n",
      "Time taken:  0:01:31.052472 d= 7 .........\n",
      "Time taken:  0:02:17.617634 d= 7 .........\n",
      "Time taken:  0:01:28.670462 d= 7 .........\n",
      "Time taken:  0:01:46.653437 d= 7 .........\n",
      "Time taken:  0:01:33.594603 d= 7 .........\n",
      "Time taken:  0:01:29.449050 d= 7 .........\n",
      "Time taken:  0:01:31.776813 d= 7 .........\n",
      "Time taken:  0:01:29.493462 d= 7 .........\n",
      "Time taken:  0:01:31.402634 d= 7 .........\n"
     ]
    }
   ],
   "source": [
    "d_arr = np.arange(1,8)\n",
    "runs = 20\n",
    "training_set_errors = np.zeros((len(d_arr),runs))\n",
    "test_set_errors = np.zeros((len(d_arr),runs))\n",
    "\n",
    "for d in d_arr:\n",
    "    for i in range(runs):\n",
    "        startTime = datetime.now()\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "        alphas,train_errors = perceptron_train_ovo(X_train,y_train, d)\n",
    "        predictions, test_error = perceptron_test_ovo(X_test,X_train, y_test, alphas,d)\n",
    "        \n",
    "        training_set_errors[d-1, i] = train_errors[-1]\n",
    "        test_set_errors[d-1, i] = test_error\n",
    "        print(\"Now doing run \", i, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "        print(\"Time taken: \", datetime.now() - startTime )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>244.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   \\\n",
       "0  244.0  253.0  273.0  277.0  249.0  246.0  247.0  244.0  229.0  265.0   \n",
       "1  144.0  146.0  160.0  158.0  153.0  171.0  160.0  162.0  143.0  162.0   \n",
       "2  119.0  135.0  101.0  127.0  114.0  132.0  135.0  103.0  119.0  126.0   \n",
       "3  103.0   93.0  101.0  108.0  102.0  106.0  112.0  117.0   98.0  103.0   \n",
       "4   83.0  108.0   85.0   93.0  120.0   85.0   85.0  102.0   92.0  103.0   \n",
       "5   70.0   87.0  103.0   84.0   85.0   80.0   82.0   99.0   99.0   95.0   \n",
       "6   90.0   77.0   78.0   96.0   98.0   99.0   96.0  110.0   94.0   78.0   \n",
       "\n",
       "      10     11     12     13     14     15     16     17     18     19  \n",
       "0  245.0  283.0  261.0  270.0  246.0  263.0  245.0  260.0  230.0  279.0  \n",
       "1  150.0  152.0  127.0  150.0  183.0  151.0  156.0  152.0  173.0  169.0  \n",
       "2  128.0  109.0  110.0  139.0  117.0  117.0  106.0  135.0  110.0  117.0  \n",
       "3  105.0  100.0   95.0   86.0  107.0  107.0   88.0  108.0  104.0  100.0  \n",
       "4   91.0   97.0  100.0   94.0   91.0   82.0  128.0   79.0   98.0  103.0  \n",
       "5   84.0  103.0   92.0   82.0   92.0   93.0   97.0  103.0   86.0   92.0  \n",
       "6   97.0   96.0   83.0   92.0   90.0   99.0   95.0   75.0   90.0   82.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_set_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_set_errors).to_pickle('test_errors_basic_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training_set_errors).to_pickle('train_errors_basic_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1327 +- 0.0041</td>\n",
       "      <td>0.1373 +- 0.0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0637 +- 0.0032</td>\n",
       "      <td>0.0839 +- 0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0366 +- 0.0031</td>\n",
       "      <td>0.0645 +- 0.0061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0228 +- 0.0018</td>\n",
       "      <td>0.0549 +- 0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0176 +- 0.0019</td>\n",
       "      <td>0.0516 +- 0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0125 +- 0.0015</td>\n",
       "      <td>0.0486 +- 0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0104 +- 0.0018</td>\n",
       "      <td>0.0488 +- 0.0048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training set error rate Test set error rate\n",
       "1        0.1327 +- 0.0041    0.1373 +- 0.0082\n",
       "2        0.0637 +- 0.0032    0.0839 +- 0.0065\n",
       "3        0.0366 +- 0.0031    0.0645 +- 0.0061\n",
       "4        0.0228 +- 0.0018    0.0549 +- 0.0040\n",
       "5        0.0176 +- 0.0019    0.0516 +- 0.0066\n",
       "6        0.0125 +- 0.0015    0.0486 +- 0.0047\n",
       "7        0.0104 +- 0.0018    0.0488 +- 0.0048"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_std = []\n",
    "for d in d_arr:\n",
    "    data_t = []\n",
    "    data_t.append(\"{0:.4f} +- {1:.4f}\".format(training_set_errors[d-1].mean() / len(y_train), \\\n",
    "                                            np.std(training_set_errors[d-1]) / len(y_train)))\n",
    "    data_t.append(\"{0:.4f} +- {1:.4f}\".format(test_set_errors[d-1].mean() / len(y_test), \\\n",
    "                                            np.std(test_set_errors[d-1]) / len(y_test)))\n",
    "    means_std.append(data_t)\n",
    "    \n",
    "df = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#means_std = construct_dataframe_error_rates(training_set_errors, test_set_errors)\n",
    "#df2 = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df).to_pickle('basic_ovo') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation - Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Validation of One vs. One\n",
    "def cross_validation_ovo(matrix,d,k=5,kernel_choice='Polynomial'):\n",
    "    #np.random.shuffle(matrix)\n",
    "    kf = KFold(n_splits=k)\n",
    "    error_cv_arr = np.zeros(k)\n",
    "    i=0\n",
    "    \n",
    "    for train_index, cv_index in kf.split(matrix):\n",
    "        # Spit the matrix using the indices gained by the CV method and construct X and Y arrays\n",
    "        matrix_train, matrix_cv = matrix[train_index], matrix[cv_index]\n",
    "\n",
    "        X_train = matrix_train[:,0:-1]\n",
    "        X_cv = matrix_cv[:,0:-1]\n",
    "        y_train = matrix_train[:,-1] \n",
    "        y_cv = matrix_cv[:,-1]\n",
    "\n",
    "        # We are only interested in the alphas and not the MSE on the training set\n",
    "        alphas, train_errors = perceptron_train_ovo(X_train,y_train, d, kernel_choice)\n",
    "        cv_errors,predictions = perceptron_test_ovo(X_cv, X_train, y_cv, alphas, d, kernel_choice)\n",
    "        error_cv_arr[i] = cv_errors\n",
    "        i += 1\n",
    "        \n",
    "        return error_cv_arr.mean(), (error_cv_arr.var())**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_process_ovo(data, d_arr, runs, kernel_choice, calculate_confusions):\n",
    "    d_stars = np.zeros(runs)\n",
    "    test_errors = np.zeros(runs)\n",
    "    confusions = []\n",
    "    mistakes_per_run = np.zeros(x.shape[0])\n",
    "\n",
    "    for j in range(runs):\n",
    "        np.random.shuffle(data)\n",
    "        confusion = np.zeros((10, 10))\n",
    "\n",
    "        print(\"WARNING: Change the number of runs to 20!!!\")\n",
    "        # In each run we will iterate through the d array and use all possible values of d\n",
    "\n",
    "        # Allocate 80/20 percent for training and test set\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5)\n",
    "\n",
    "        CV_means = np.zeros(len(d_arr))\n",
    "        \n",
    "        for i in range(len(d_arr)):\n",
    "            print(\"d=\", d_arr[i])\n",
    "            print(\"Now doing run \", j+1, \"/\", runs, \" for d=\", d_arr[i], \".........\", end='\\r')\n",
    "            errors_CV_mean, _ = cross_validation_ovo(data,d_arr[i],k=5,kernel_choice='Polynomial')\n",
    "            print(\"CV mean error=\", errors_CV_mean )\n",
    "            CV_means[i] = errors_CV_mean\n",
    "            print('CV mean errors=', errors_CV_mean)\n",
    "            \n",
    "\n",
    "        # Train in whole 80% now with d_star\n",
    "        d_stars[j] = d_arr[CV_means.argmin()]\n",
    "        print('d_stars',d_stars[j])\n",
    "        alphas, errors = perceptron_train_ovo(X_train, y_train, d_stars[j], kernel_choice = kernel_choice)\n",
    "\n",
    "        mistakes,_ = perceptron_test_ovo(X_test, X_train, y_test, alphas, d_stars[j], kernel_choice = kernel_choice)\n",
    "        test_errors[j] = mistakes / len(y_test)\n",
    "        \n",
    "        if calculate_confusions:\n",
    "            # Test in all the data set, so that we know which ones\n",
    "            # are the \"toughest\" to predict in the whole data set. We can't really just do it on \n",
    "            # either the training or test set, as it is randomly split so order will not be pertained.\n",
    "            _, preds_all, confidences = perceptron_test_ovo(x, X_train, y, alphas, d_stars[j], kernel_choice = kernel_choice)\n",
    "            for i in range(x.shape[0]):\n",
    "                pred_label = preds_all[i].argmax()\n",
    "                if pred_label != y[i]:\n",
    "                    confusion[int(y[i]), pred_label] += 1\n",
    "                    mistakes_per_run[i] += 1\n",
    "\n",
    "            confusions.append(confusion)\n",
    "    return d_stars, test_errors, confusions, mistakes_per_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some problems to be fixed in cross validation function: same error for all values of d?\n",
    "runs = 2\n",
    "d_arr = np.arange(1,8)\n",
    "d_stars, test_errors, confusions, mistakes_per_run = cv_process_ovo(data, d_arr, runs, 'Polynomial', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
