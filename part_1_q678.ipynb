{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approaches to Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#link = \"http://www0.cs.ucl.ac.uk/staff/M.Herbster/SL/misc/zipcombo.dat\"\n",
    "filename = 'zipcombo.dat'\n",
    "#urllib.request.urlretrieve(link, filename)\n",
    "data = np.loadtxt(filename)     # read numpy array from fil\n",
    "#data = np.loadtxt(filename)     #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data[:,0]\n",
    "x = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 - Alternative generalization method, One vs. One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allocate_training_test_sets(data,r =1/5, random_split=False):\n",
    "    if random_split:\n",
    "        np.random.shuffle(data)\n",
    "    X= data[:,1:]\n",
    "    y= data[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=r)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def add_bias(x):\n",
    "    x_with_bias = np.ones((x.shape[0],x.shape[1]+1))\n",
    "    x_with_bias[:,:-1] = x\n",
    "    return x_with_bias\n",
    "\n",
    "\n",
    "def calculate_kernel_double(x1, x2, d, kernel_choice):\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x1,x2,d)\n",
    "#         print(\"Constructed a Polynomial kernel for testing\")\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_double(x1, x2)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "#         print(\"Constructed a Gaussian kernel for testing\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "\n",
    "\n",
    "def calculate_kernel_single(x, d, kernel_choice):\n",
    "    if kernel_choice=='Polynomial':\n",
    "        K_train = Polynomial_Kernel(x,x,d)\n",
    "#         print(\"Constructed a Polynomial kernel for training\")\n",
    "    elif kernel_choice=='Gaussian':\n",
    "        pairwise_distances = pairwise_distance_single(x)\n",
    "        K_train = Gaussian_Kernel(pairwise_distances,c=d)\n",
    "#         print(\"Constructed a Gaussian kernel for training\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported value for kernel. Supported values: Polynomial, Gaussian\")\n",
    "    return K_train\n",
    "\n",
    "\n",
    "#Discuss the use of the this kernel. i.e. talk about non-linear seperability. \n",
    "def Polynomial_Kernel(x1,x2,d):\n",
    "    K = (x1 @ x2.T)**d\n",
    "    return K\n",
    "\n",
    "def pairwise_distance_single(X): # distances of X training data, single X matrix\n",
    "    m =X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    G = np.matmul(X,X.T)\n",
    "    DG = np.diag(G).reshape(G.shape[0],1)\n",
    "    distances_sq = np.matmul(DG,np.ones((G.shape[0],1)).T)+ np.matmul(np.ones((G.shape[1],1)),DG.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def pairwise_distance_double(X1,X2): # distances of X training data, double matrices, X1 and X2\n",
    "    X1_pow = (X1**2).sum(axis=1).reshape(X1.shape[0],1) #sum the rows, size m1 array\n",
    "    X2_pow = (X2**2).sum(axis=1).reshape(X2.shape[0],1) #sum the rows, size m2 array\n",
    "    G = np.matmul(X1,X2.T)\n",
    "    m1,m2 =G.shape[0],G.shape[1] \n",
    "    distances_sq = np.matmul(X1_pow,np.ones((m2,1)).T)+ np.matmul(np.ones((m1,1)),X2_pow.T)-2.0*G\n",
    "    return distances_sq\n",
    "\n",
    "def Gaussian_Kernel(distances_sq,c=1):\n",
    "    K = np.exp(-c*distances_sq)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs. One Pairwise Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One vs. one approach: train k*(k-1)/2 binary classifiers to identify k classes\n",
    "#For example, one classifier could be trained to distinguish between digit 0 and digit 1.\n",
    "#Consider symmetry when computing prediction\n",
    "\n",
    "#Write a function that trains a binary classifier given two classes, given kernel:\n",
    "def classifier_ovo(class1,class2,K,alpha_ovo,iter_num):\n",
    "    vote = np.sign(((alpha_ovo[:].T @K[iter_num,:]).T))\n",
    "    return vote #returns a vote, within (-1,1)\n",
    "\n",
    "def perceptron_train_ovo(x,y,d=2,kernel_choice='Polynomial',max_epoch=10, tol=0.01):\n",
    "    m = x.shape[0] #number of examples\n",
    "    n = x.shape[1] #number of features\n",
    "    classes_num = 10 #number of classes \n",
    "    error_per_epoch = np.zeros(max_epoch)\n",
    "    errors = np.zeros(m)\n",
    "    K_train = calculate_kernel_single(x, d, kernel_choice)\n",
    "    \n",
    "    num_errors = 0 \n",
    "    alpha = np.zeros((m,classes_num,classes_num)) \n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        errors = np.zeros(m)\n",
    "        num_errors = 0 #This should be bounded..? Maybe calculate the bound in the explanation\n",
    "    \n",
    "        #iterate through training set\n",
    "        for t in range(m):\n",
    "            if t<1:\n",
    "                alpha_prev = alpha[0,:,:] #when t=0, the previous alpha is set to be 0\n",
    "            else:\n",
    "                alpha_prev = alpha[t-1,:,:] #\n",
    "\n",
    "            x_t = x[t,:]\n",
    "            y_t = y[t]\n",
    "\n",
    "            votes_board = np.zeros((classes_num, classes_num)) #zero on the horizontal. \n",
    "            classes_list = np.array(range(classes_num))\n",
    "\n",
    "\n",
    "            for i in range(classes_num):\n",
    "                c1 = classes_list[i]\n",
    "                classes_rest = classes_list[classes_list>c1]\n",
    "                for j in range(len(classes_rest)):\n",
    "                    c2 = classes_rest[j]\n",
    "                    alpha_ovo = alpha[:,c1,c2]\n",
    "                    vote = classifier_ovo(c1,c2,K_train,alpha_ovo,iter_num=t)\n",
    "                    votes_board[c1,c2] = vote\n",
    "\n",
    "            #Count the votes in the board\n",
    "            votes_count = votes_board.sum(axis=0)\n",
    "            pred_t = votes_count.argmax()\n",
    "\n",
    "            if pred_t!=y_t:\n",
    "                num_errors +=1\n",
    "\n",
    "                #increase alpha for all the positive classifier of the correct label.\n",
    "                #decrease alpha for the negative classifier of the false label. \n",
    "                alpha_t = alpha_prev #initialize it to its previous form\n",
    "                alpha_t[:,int(y_t)] =+1 # column belonging to correct label class +=1\n",
    "                alpha_t[:,int(pred_t)] =-1 # column belonging to false predicted class -=1\n",
    "\n",
    "                #store alpha_t into the matrix for future reference\n",
    "                alpha[t,:,:] = alpha_t\n",
    "            \n",
    "            errors[t] = num_errors \n",
    "        \n",
    "        error_per_epoch[epoch] = errors[-1]\n",
    "        #print(epoch, 'error=',errors[-1])\n",
    "        \n",
    "        if epoch>1:\n",
    "            diff_rates = (error_per_epoch[epoch-1] - error_per_epoch[epoch])/m\n",
    "            \n",
    "            #Stop if the error rate has increased, \n",
    "            #or the difference in error rate between the previous one and the current one < tolerance. \n",
    "            if diff_rates<tol or diff_rates<0:\n",
    "                print('difference in error rate', diff_rates)\n",
    "                print('break point at epoch=', epoch )\n",
    "                break\n",
    "        \n",
    "    return alpha, error_per_epoch[:epoch+1]\n",
    "\n",
    "\n",
    "\n",
    "def perceptron_test_ovo(x_test,x_train,y_test,alphas, d,kernel_choice='Polynomial'):\n",
    "    m_test = x_test.shape[0]\n",
    "    m_train = x_train.shape[0]\n",
    "    \n",
    "    K_test = calculate_kernel_double(x_train, x_test, d, kernel_choice)\n",
    "    \n",
    "    classes_num = 10\n",
    "    classes_list = np.array(range(classes_num))\n",
    "    votes_ovo =np.zeros((m_test,10,10))\n",
    "    \n",
    "    for i in range(classes_num):\n",
    "        c1 = classes_list[i]\n",
    "        classes_rest = classes_list[classes_list>c1]\n",
    "        for j in range(len(classes_rest)):\n",
    "            c2 = classes_rest[j]\n",
    "            alphas_ovo_c1c2 = alphas[:,int(c1),int(c2)]\n",
    "            vote = np.sign(alphas_ovo_c1c2.T@K_test) \n",
    "            votes_ovo[:,c1,c2] = vote\n",
    "                \n",
    "    sum_votes = np.sum(votes_ovo,axis=1)\n",
    "    pred = sum_votes.argmax(axis=1)\n",
    "    diff = pred - y_test\n",
    "    mistakes = len(diff[diff!=0])\n",
    "    \n",
    "    return mistakes,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dataframe_error_rates(training_set_errors, test_set_errors):\n",
    "    means_std = []\n",
    "    for d in d_arr:\n",
    "        data_t = []\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(training_set_errors[d-1].mean(), \\\n",
    "                                                np.std(training_set_errors[d-1])))\n",
    "        data_t.append(\"{0:.4f} +- {1:.4f}\".format(test_set_errors[d-1].mean(), \\\n",
    "                                                np.std(test_set_errors[d-1])))\n",
    "        means_std.append(data_t)\n",
    "    return means_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_results(data,d_arr, kernel_choice, runs):\n",
    "    training_set_errors = np.zeros((len(d_arr),runs))\n",
    "    test_set_errors = np.zeros((len(d_arr),runs))\n",
    "    for d in d_arr:\n",
    "        for i in range(runs):\n",
    "            print(\"Now doing run \", i+1, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "            X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5,random_split=True)\n",
    "            \n",
    "            alphas,train_errors = perceptron_train_ovo(X_train,y_train, d)\n",
    "            test_errors, predictions = perceptron_test_ovo(X_test,X_train, y_test, alphas,d)\n",
    "\n",
    "            training_set_errors[d-1, i] = train_errors[-1] / len(y_train)\n",
    "            test_set_errors[d-1, i] = test_errors / len(y_test)\n",
    "    return training_set_errors, test_set_errors\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on all training data and test data, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 error= 2043.0\n",
      "1 error= 1248.0\n",
      "2 error= 1133.0\n",
      "3 error= 1103.0\n",
      "difference in error rate 0.004033342296316214\n",
      "break point at epoch= 3\n",
      "0 error= 1992.0\n",
      "1 error= 1274.0\n",
      "2 error= 1133.0\n",
      "3 error= 1040.0\n",
      "4 error= 1021.0\n",
      "difference in error rate 0.002554450121000269\n",
      "break point at epoch= 4\n",
      "0 error= 1416.0\n",
      "1 error= 693.0\n",
      "2 error= 558.0\n",
      "3 error= 543.0\n",
      "difference in error rate 0.002016671148158107\n",
      "break point at epoch= 3\n",
      "0 error= 1464.0\n",
      "1 error= 679.0\n",
      "2 error= 579.0\n",
      "3 error= 540.0\n",
      "difference in error rate 0.005243344985211078\n",
      "break point at epoch= 3\n"
     ]
    }
   ],
   "source": [
    "train_e, test_e =basic_results(data,[1,2], 'polynomial', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in error rate 0.009948910997579994\n",
      "break point at epoch= 2\n",
      "0:00:36.404018\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "X_train, X_test, y_train, y_test = allocate_training_test_sets(data,r =1/5)\n",
    "alphas_ovo, train_errors = perceptron_train_ovo(X_train,y_train, d=2,max_epoch=10)\n",
    "test_errors,predictions = perceptron_test_ovo(X_test, X_train, y_test, alphas_ovo, 2)\n",
    "print(datetime.now() - startTime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Results  - Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:02:06.850727d= 1 .........\n",
      "Time taken:  0:01:34.001226d= 1 .........\n",
      "Time taken:  0:01:30.258883d= 1 .........\n",
      "Time taken:  0:01:31.067757d= 1 .........\n",
      "Time taken:  0:01:45.497868d= 1 .........\n",
      "Time taken:  0:01:29.221478d= 1 .........\n",
      "Time taken:  0:01:29.647567d= 1 .........\n",
      "Time taken:  0:37:47.489077d= 1 .........\n",
      "Time taken:  0:02:08.405380d= 1 .........\n",
      "Time taken:  0:02:11.632202d= 1 .........\n",
      "Time taken:  0:02:20.107663 d= 1 .........\n",
      "Time taken:  0:01:36.432795 d= 1 .........\n",
      "Time taken:  0:01:31.106984 d= 1 .........\n",
      "Time taken:  0:01:41.431903 d= 1 .........\n",
      "Time taken:  0:02:05.996656 d= 1 .........\n",
      "Time taken:  0:01:37.294993 d= 1 .........\n",
      "Time taken:  0:01:47.028888 d= 1 .........\n",
      "Time taken:  0:02:17.003802 d= 1 .........\n",
      "Time taken:  0:02:00.293113 d= 1 .........\n",
      "Time taken:  0:01:43.101110 d= 1 .........\n",
      "Time taken:  0:01:50.043767d= 2 .........\n",
      "Time taken:  0:02:00.766049d= 2 .........\n",
      "Time taken:  0:01:31.958548d= 2 .........\n",
      "Time taken:  0:01:48.359575d= 2 .........\n",
      "Time taken:  0:01:53.189985d= 2 .........\n",
      "Time taken:  0:02:17.467815d= 2 .........\n",
      "Time taken:  0:01:58.354250d= 2 .........\n",
      "Time taken:  0:02:16.403180d= 2 .........\n",
      "Time taken:  0:02:26.140234d= 2 .........\n",
      "Time taken:  0:01:55.932878d= 2 .........\n",
      "Time taken:  0:01:49.616222 d= 2 .........\n",
      "Time taken:  0:01:50.513306 d= 2 .........\n",
      "Time taken:  0:01:53.009829 d= 2 .........\n",
      "Time taken:  0:01:49.357510 d= 2 .........\n",
      "Time taken:  0:02:16.144103 d= 2 .........\n",
      "Time taken:  0:02:39.298476 d= 2 .........\n",
      "Time taken:  0:02:10.666657 d= 2 .........\n",
      "Time taken:  0:02:10.121578 d= 2 .........\n",
      "Time taken:  0:02:25.695316 d= 2 .........\n",
      "Time taken:  0:02:30.826478 d= 2 .........\n",
      "Time taken:  0:01:46.805240d= 3 .........\n",
      "Time taken:  0:01:51.656468d= 3 .........\n",
      "Time taken:  0:01:43.690318d= 3 .........\n",
      "Time taken:  0:01:41.586181d= 3 .........\n",
      "Time taken:  0:01:28.500553d= 3 .........\n",
      "Time taken:  0:01:58.240910d= 3 .........\n",
      "Time taken:  0:01:52.754168d= 3 .........\n",
      "Time taken:  0:01:27.862636d= 3 .........\n",
      "Time taken:  0:01:44.105281d= 3 .........\n",
      "Time taken:  0:01:30.130777d= 3 .........\n",
      "Time taken:  0:02:00.501362 d= 3 .........\n",
      "Time taken:  0:02:53.273166 d= 3 .........\n",
      "Time taken:  0:02:27.548324 d= 3 .........\n",
      "Time taken:  0:01:55.743130 d= 3 .........\n",
      "Time taken:  0:01:47.774720 d= 3 .........\n",
      "Time taken:  0:01:36.442807 d= 3 .........\n",
      "Time taken:  0:01:44.653598 d= 3 .........\n",
      "Time taken:  0:01:48.296969 d= 3 .........\n",
      "Time taken:  0:02:05.146101 d= 3 .........\n",
      "Time taken:  0:01:28.551091 d= 3 .........\n",
      "Time taken:  0:01:37.524749d= 4 .........\n",
      "Time taken:  0:01:27.574453d= 4 .........\n",
      "Time taken:  0:01:28.292787d= 4 .........\n",
      "Time taken:  0:01:27.942754d= 4 .........\n",
      "Time taken:  0:01:28.632976d= 4 .........\n",
      "Time taken:  2:01:43.840124d= 4 .........\n",
      "Time taken:  0:41:58.816528d= 4 .........\n",
      "Time taken:  6:01:37.842771d= 4 .........\n",
      "Time taken:  2:07:53.915923d= 4 .........\n",
      "Time taken:  0:01:34.436600d= 4 .........\n",
      "Time taken:  0:03:27.632577 d= 4 .........\n",
      "Time taken:  0:02:47.286837 d= 4 .........\n",
      "Time taken:  0:01:51.324462 d= 4 .........\n",
      "Time taken:  0:02:57.404245 d= 4 .........\n",
      "Time taken:  0:02:13.399258 d= 4 .........\n",
      "Time taken:  0:02:14.520158 d= 4 .........\n",
      "Time taken:  0:02:13.481843 d= 4 .........\n",
      "Time taken:  0:02:07.450351 d= 4 .........\n",
      "Time taken:  0:01:59.674105 d= 4 .........\n",
      "Time taken:  0:02:24.406137 d= 4 .........\n",
      "Time taken:  0:02:17.549034d= 5 .........\n",
      "Time taken:  0:02:43.453675d= 5 .........\n",
      "Time taken:  0:03:06.719847d= 5 .........\n",
      "Time taken:  0:01:55.205223d= 5 .........\n",
      "Time taken:  0:02:19.244408d= 5 .........\n",
      "Time taken:  0:02:05.441542d= 5 .........\n",
      "Time taken:  0:01:56.106333d= 5 .........\n",
      "Time taken:  0:01:44.056123d= 5 .........\n",
      "Time taken:  0:01:40.267306d= 5 .........\n",
      "Time taken:  0:01:46.268214d= 5 .........\n",
      "Time taken:  0:01:46.394773 d= 5 .........\n",
      "Time taken:  0:01:46.734112 d= 5 .........\n",
      "Time taken:  0:02:19.535669 d= 5 .........\n",
      "Time taken:  0:02:12.995980 d= 5 .........\n",
      "Time taken:  0:01:40.640696 d= 5 .........\n",
      "Time taken:  0:01:49.781665 d= 5 .........\n",
      "Time taken:  0:02:16.134023 d= 5 .........\n",
      "Time taken:  0:01:42.875539 d= 5 .........\n",
      "Time taken:  0:03:02.054181 d= 5 .........\n",
      "Time taken:  0:02:32.294809 d= 5 .........\n",
      "Time taken:  0:01:41.519595d= 6 .........\n",
      "Time taken:  0:03:46.208583d= 6 .........\n",
      "Time taken:  0:02:18.865883d= 6 .........\n",
      "Time taken:  0:02:18.113948d= 6 .........\n",
      "Time taken:  0:02:01.599529d= 6 .........\n",
      "Time taken:  0:02:06.558015d= 6 .........\n",
      "Time taken:  0:01:58.664370d= 6 .........\n",
      "Time taken:  0:02:19.882227d= 6 .........\n",
      "Time taken:  0:02:45.799483d= 6 .........\n",
      "Time taken:  0:02:26.759261d= 6 .........\n",
      "Time taken:  0:02:08.920570 d= 6 .........\n",
      "Time taken:  0:02:22.742971 d= 6 .........\n",
      "Time taken:  0:02:24.041303 d= 6 .........\n",
      "Time taken:  0:01:51.073926 d= 6 .........\n",
      "Time taken:  0:02:34.833052 d= 6 .........\n",
      "Time taken:  0:02:22.257708 d= 6 .........\n",
      "Time taken:  0:03:04.623419 d= 6 .........\n",
      "Time taken:  0:03:46.692598 d= 6 .........\n",
      "Time taken:  0:02:36.150449 d= 6 .........\n",
      "Time taken:  0:02:55.663773 d= 6 .........\n",
      "Time taken:  0:02:37.955965d= 7 .........\n",
      "Time taken:  0:03:12.572248d= 7 .........\n",
      "Time taken:  0:03:15.551220d= 7 .........\n",
      "Time taken:  0:02:51.438215d= 7 .........\n",
      "Time taken:  0:02:09.583653d= 7 .........\n",
      "Time taken:  0:02:14.031521d= 7 .........\n",
      "Time taken:  0:01:27.567330d= 7 .........\n",
      "Time taken:  0:01:50.295126d= 7 .........\n",
      "Time taken:  0:01:30.063281d= 7 .........\n",
      "Time taken:  0:01:31.076848d= 7 .........\n",
      "Time taken:  0:01:30.178759 d= 7 .........\n",
      "Time taken:  0:01:31.052472 d= 7 .........\n",
      "Time taken:  0:02:17.617634 d= 7 .........\n",
      "Time taken:  0:01:28.670462 d= 7 .........\n",
      "Time taken:  0:01:46.653437 d= 7 .........\n",
      "Time taken:  0:01:33.594603 d= 7 .........\n",
      "Time taken:  0:01:29.449050 d= 7 .........\n",
      "Time taken:  0:01:31.776813 d= 7 .........\n",
      "Time taken:  0:01:29.493462 d= 7 .........\n",
      "Time taken:  0:01:31.402634 d= 7 .........\n"
     ]
    }
   ],
   "source": [
    "d_arr = np.arange(1,8)\n",
    "runs = 20\n",
    "training_set_errors = np.zeros((len(d_arr),runs))\n",
    "test_set_errors = np.zeros((len(d_arr),runs))\n",
    "\n",
    "for d in d_arr:\n",
    "    for i in range(runs):\n",
    "        startTime = datetime.now()\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5,random_split=True)\n",
    "        alphas,train_errors = perceptron_train_ovo(X_train,y_train, d)\n",
    "        predictions, test_error = perceptron_test_ovo(X_test,X_train, y_test, alphas,d)\n",
    "        \n",
    "        training_set_errors[d-1, i] = train_errors[-1]\n",
    "        test_set_errors[d-1, i] = test_error\n",
    "        print(\"Now doing run \", i, \"/\", runs, \" for d=\", d,\".........\", end='\\r')\n",
    "        print(\"Time taken: \", datetime.now() - startTime )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>244.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   \\\n",
       "0  244.0  253.0  273.0  277.0  249.0  246.0  247.0  244.0  229.0  265.0   \n",
       "1  144.0  146.0  160.0  158.0  153.0  171.0  160.0  162.0  143.0  162.0   \n",
       "2  119.0  135.0  101.0  127.0  114.0  132.0  135.0  103.0  119.0  126.0   \n",
       "3  103.0   93.0  101.0  108.0  102.0  106.0  112.0  117.0   98.0  103.0   \n",
       "4   83.0  108.0   85.0   93.0  120.0   85.0   85.0  102.0   92.0  103.0   \n",
       "5   70.0   87.0  103.0   84.0   85.0   80.0   82.0   99.0   99.0   95.0   \n",
       "6   90.0   77.0   78.0   96.0   98.0   99.0   96.0  110.0   94.0   78.0   \n",
       "\n",
       "      10     11     12     13     14     15     16     17     18     19  \n",
       "0  245.0  283.0  261.0  270.0  246.0  263.0  245.0  260.0  230.0  279.0  \n",
       "1  150.0  152.0  127.0  150.0  183.0  151.0  156.0  152.0  173.0  169.0  \n",
       "2  128.0  109.0  110.0  139.0  117.0  117.0  106.0  135.0  110.0  117.0  \n",
       "3  105.0  100.0   95.0   86.0  107.0  107.0   88.0  108.0  104.0  100.0  \n",
       "4   91.0   97.0  100.0   94.0   91.0   82.0  128.0   79.0   98.0  103.0  \n",
       "5   84.0  103.0   92.0   82.0   92.0   93.0   97.0  103.0   86.0   92.0  \n",
       "6   97.0   96.0   83.0   92.0   90.0   99.0   95.0   75.0   90.0   82.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_set_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_set_errors).to_pickle('test_errors_basic_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(training_set_errors).to_pickle('train_errors_basic_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set error rate</th>\n",
       "      <th>Test set error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1327 +- 0.0041</td>\n",
       "      <td>0.1373 +- 0.0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0637 +- 0.0032</td>\n",
       "      <td>0.0839 +- 0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0366 +- 0.0031</td>\n",
       "      <td>0.0645 +- 0.0061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0228 +- 0.0018</td>\n",
       "      <td>0.0549 +- 0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0176 +- 0.0019</td>\n",
       "      <td>0.0516 +- 0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0125 +- 0.0015</td>\n",
       "      <td>0.0486 +- 0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0104 +- 0.0018</td>\n",
       "      <td>0.0488 +- 0.0048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Training set error rate Test set error rate\n",
       "1        0.1327 +- 0.0041    0.1373 +- 0.0082\n",
       "2        0.0637 +- 0.0032    0.0839 +- 0.0065\n",
       "3        0.0366 +- 0.0031    0.0645 +- 0.0061\n",
       "4        0.0228 +- 0.0018    0.0549 +- 0.0040\n",
       "5        0.0176 +- 0.0019    0.0516 +- 0.0066\n",
       "6        0.0125 +- 0.0015    0.0486 +- 0.0047\n",
       "7        0.0104 +- 0.0018    0.0488 +- 0.0048"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_std = []\n",
    "for d in d_arr:\n",
    "    data_t = []\n",
    "    data_t.append(\"{0:.4f} +- {1:.4f}\".format(training_set_errors[d-1].mean() / len(y_train), \\\n",
    "                                            np.std(training_set_errors[d-1]) / len(y_train)))\n",
    "    data_t.append(\"{0:.4f} +- {1:.4f}\".format(test_set_errors[d-1].mean() / len(y_test), \\\n",
    "                                            np.std(test_set_errors[d-1]) / len(y_test)))\n",
    "    means_std.append(data_t)\n",
    "    \n",
    "df = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#means_std = construct_dataframe_error_rates(training_set_errors, test_set_errors)\n",
    "#df2 = pd.DataFrame(data=means_std, index=d_arr, columns=['Training set error rate', 'Test set error rate'])\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df).to_pickle('basic_ovo') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation - Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Validation of One vs. One\n",
    "#matrix is the training dataset. \n",
    "def cross_validation_ovo(matrix,d,k=5,kernel_choice='Polynomial'):\n",
    "    #np.random.shuffle(matrix)\n",
    "    kf = KFold(n_splits=k)\n",
    "    error_cv_arr = np.zeros(k)\n",
    "    i=0\n",
    "    \n",
    "    for train_index, cv_index in kf.split(matrix):\n",
    "        # Spit the matrix using the indices gained by the CV method and construct X and Y arrays\n",
    "        matrix_train, matrix_cv = matrix[train_index], matrix[cv_index]\n",
    "\n",
    "        X_train = matrix_train[:,1:]\n",
    "        X_cv = matrix_cv[:,1:]\n",
    "        y_train = matrix_train[:,0] \n",
    "        y_cv = matrix_cv[:,0]\n",
    "\n",
    "        # We are only interested in the alphas and not the MSE on the training set\n",
    "        alphas, train_errors = perceptron_train_ovo(X_train,y_train, d, kernel_choice)\n",
    "        cv_errors,predictions = perceptron_test_ovo(X_cv, X_train, y_cv, alphas, d, kernel_choice)\n",
    "        \n",
    "        #print('cv_errors=',cv_errors, ' for d=',d)\n",
    "        error_cv_arr[i] = cv_errors\n",
    "        i += 1\n",
    "        \n",
    "        return error_cv_arr.mean(), (error_cv_arr.var())**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_process_ovo(data, d_arr, runs, kernel_choice, calculate_confusions):\n",
    "    d_stars = np.zeros(runs)\n",
    "    test_errors = np.zeros(runs)\n",
    "    confusions = []\n",
    "    mistakes_per_run = np.zeros(x.shape[0])\n",
    "\n",
    "    for j in range(runs):\n",
    "        confusion = np.zeros((10, 10))\n",
    "        print('run=',j)\n",
    "        #print(\"WARNING: Change the number of runs to 20!!!\")\n",
    "        # In each run we will iterate through the d array and use all possible values of d\n",
    "\n",
    "        # Allocate 80/20 percent for training and test set\n",
    "        X_train, X_test, y_train, y_test = allocate_training_test_sets(data, r=1/5, random_split=True)\n",
    "        data_train = np.zeros((X_train.shape[0],X_train.shape[1]+1))\n",
    "        data_train[:,0] = y_train\n",
    "        data_train[:,1:] = X_train\n",
    "        \n",
    "        CV_means = np.zeros(len(d_arr))\n",
    "        #Only apply cross validation on training data\n",
    "        for i in range(len(d_arr)):\n",
    "            print(\"d=\", d_arr[i])\n",
    "            #print(\"Now doing run \", j+1, \"/\", runs, \" for d=\", d_arr[i], \".........\", end='\\r')\n",
    "            errors_CV_mean, _ = cross_validation_ovo(data_train, d_arr[i], k=5, kernel_choice='Polynomial')\n",
    "            #print(\"CV mean error=\", errors_CV_mean)\n",
    "            CV_means[i] = errors_CV_mean  \n",
    "\n",
    "        # Train in whole 80% now with d_star\n",
    "        d_stars[j] = d_arr[CV_means.argmin()]\n",
    "        #print('d_stars',d_stars[j])\n",
    "        alphas, errors = perceptron_train_ovo(X_train, y_train, d_stars[j], kernel_choice = kernel_choice)\n",
    "\n",
    "        mistakes,_ = perceptron_test_ovo(X_test, X_train, y_test, alphas, d_stars[j], kernel_choice = kernel_choice)\n",
    "        test_errors[j] = mistakes / len(y_test)\n",
    "        \n",
    "        if calculate_confusions:\n",
    "            # Test in all the data set, so that we know which ones\n",
    "            # are the \"toughest\" to predict in the whole data set. We can't really just do it on \n",
    "            # either the training or test set, as it is randomly split so order will not be pertained.\n",
    "            _, preds_all, confidences = perceptron_test_ovo(x, X_train, y, alphas, d_stars[j], kernel_choice = kernel_choice)\n",
    "            for i in range(x.shape[0]):\n",
    "                pred_label = preds_all[i].argmax()\n",
    "                if pred_label != y[i]:\n",
    "                    confusion[int(y[i]), pred_label] += 1\n",
    "                    mistakes_per_run[i] += 1\n",
    "\n",
    "            confusions.append(confusion)\n",
    "    return d_stars, test_errors, confusions, mistakes_per_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runs = 20\n",
    "d_arr = np.arange(1,8)\n",
    "d_stars, test_errors, confusions, mistakes_per_run = cv_process_ovo(data, d_arr, runs, 'Polynomial', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(d_stars).to_pickle('d_stars_cv_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_errors).to_pickle('test_errors_cv_ovo') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05645161, 0.04784946, 0.04462366, 0.05645161, 0.0483871 ,\n",
       "       0.04677419, 0.04569892, 0.04247312, 0.04946237, 0.0483871 ,\n",
       "       0.04731183, 0.05322581, 0.05860215, 0.05107527, 0.04086022,\n",
       "       0.05322581, 0.0483871 , 0.05      , 0.05053763, 0.04731183])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of One vs. Rest and One vs. One approaches\n",
    "\n",
    "One vs. Rest has lower training and test error than One vs. One. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
